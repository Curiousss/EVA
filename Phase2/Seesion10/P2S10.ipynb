{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2S10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Curiousss/EVA/blob/master/Phase2/Seesion10/P2S10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "d522a4ff-0aec-4c6b-a3f5-2801574e0fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/ac/a422ab8d1c57ab3f43e573b5a5f532e6afd348d81308fe66a1ecb691548e/pybullet-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 47kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "37093ab6-07fd-4998-ab1a-62933cbf28a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "c844b649-cbd3-41e8-9068-94c279d2f71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "dbb7affc-43b3-4409-d813-299d556d7dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "ecd7ee25-438b-4aa5-a153-99832b80cf6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 561 Episode Num: 1 Reward: 319.39178041529937\n",
            "Total Timesteps: 1561 Episode Num: 2 Reward: 493.98372924251254\n",
            "Total Timesteps: 2561 Episode Num: 3 Reward: 496.16666746693744\n",
            "Total Timesteps: 3561 Episode Num: 4 Reward: 524.7138556016657\n",
            "Total Timesteps: 4561 Episode Num: 5 Reward: 477.6176989938377\n",
            "Total Timesteps: 5028 Episode Num: 6 Reward: 240.99979647647856\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 156.922075\n",
            "---------------------------------------\n",
            "Total Timesteps: 5204 Episode Num: 7 Reward: 78.917906426893\n",
            "Total Timesteps: 6204 Episode Num: 8 Reward: 511.86398860211244\n",
            "Total Timesteps: 7204 Episode Num: 9 Reward: 474.3956484371958\n",
            "Total Timesteps: 8204 Episode Num: 10 Reward: 469.1386231825184\n",
            "Total Timesteps: 9204 Episode Num: 11 Reward: 503.1840477502837\n",
            "Total Timesteps: 10204 Episode Num: 12 Reward: 564.6265578827188\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 196.988032\n",
            "---------------------------------------\n",
            "Total Timesteps: 11204 Episode Num: 13 Reward: 236.95471172693755\n",
            "Total Timesteps: 12204 Episode Num: 14 Reward: 219.225108742849\n",
            "Total Timesteps: 13204 Episode Num: 15 Reward: 430.88117767520004\n",
            "Total Timesteps: 14204 Episode Num: 16 Reward: 476.2881787129559\n",
            "Total Timesteps: 15204 Episode Num: 17 Reward: 374.5315765349536\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 289.145987\n",
            "---------------------------------------\n",
            "Total Timesteps: 16204 Episode Num: 18 Reward: 307.101232013741\n",
            "Total Timesteps: 17204 Episode Num: 19 Reward: 228.09434844552683\n",
            "Total Timesteps: 18204 Episode Num: 20 Reward: 255.30087733649825\n",
            "Total Timesteps: 19204 Episode Num: 21 Reward: 142.9567659553713\n",
            "Total Timesteps: 20204 Episode Num: 22 Reward: 118.22517754440827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 385.076837\n",
            "---------------------------------------\n",
            "Total Timesteps: 21204 Episode Num: 23 Reward: 349.26712340261935\n",
            "Total Timesteps: 22204 Episode Num: 24 Reward: 208.1313040378494\n",
            "Total Timesteps: 23204 Episode Num: 25 Reward: 190.5045117067644\n",
            "Total Timesteps: 24204 Episode Num: 26 Reward: 236.3607240949849\n",
            "Total Timesteps: 25204 Episode Num: 27 Reward: 280.3142618000532\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 98.626677\n",
            "---------------------------------------\n",
            "Total Timesteps: 26204 Episode Num: 28 Reward: 97.94021489275627\n",
            "Total Timesteps: 27204 Episode Num: 29 Reward: 249.2960468183122\n",
            "Total Timesteps: 28204 Episode Num: 30 Reward: 436.12025757886437\n",
            "Total Timesteps: 29204 Episode Num: 31 Reward: 424.9942672660211\n",
            "Total Timesteps: 29872 Episode Num: 32 Reward: 300.26288777741433\n",
            "Total Timesteps: 30651 Episode Num: 33 Reward: 335.78923958098727\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 338.468166\n",
            "---------------------------------------\n",
            "Total Timesteps: 31651 Episode Num: 34 Reward: 294.54678725807815\n",
            "Total Timesteps: 32651 Episode Num: 35 Reward: 356.9646593382347\n",
            "Total Timesteps: 32671 Episode Num: 36 Reward: 2.1792630332837675\n",
            "Total Timesteps: 32691 Episode Num: 37 Reward: 1.9447096495960294\n",
            "Total Timesteps: 32711 Episode Num: 38 Reward: 2.254725725176164\n",
            "Total Timesteps: 32732 Episode Num: 39 Reward: 1.8595318494394073\n",
            "Total Timesteps: 32752 Episode Num: 40 Reward: -2.8180650350189356\n",
            "Total Timesteps: 32772 Episode Num: 41 Reward: -2.038683171102183\n",
            "Total Timesteps: 32792 Episode Num: 42 Reward: 1.2695114747684881\n",
            "Total Timesteps: 32812 Episode Num: 43 Reward: 0.14449451958553472\n",
            "Total Timesteps: 32832 Episode Num: 44 Reward: 1.686035566229942\n",
            "Total Timesteps: 32854 Episode Num: 45 Reward: 4.8199859740015984\n",
            "Total Timesteps: 33854 Episode Num: 46 Reward: 555.8277034071625\n",
            "Total Timesteps: 33874 Episode Num: 47 Reward: -2.061982786769971\n",
            "Total Timesteps: 33894 Episode Num: 48 Reward: -2.5915921115870932\n",
            "Total Timesteps: 33914 Episode Num: 49 Reward: -1.4339320567219134\n",
            "Total Timesteps: 33934 Episode Num: 50 Reward: -1.5647627745918884\n",
            "Total Timesteps: 33954 Episode Num: 51 Reward: -1.7592230966637745\n",
            "Total Timesteps: 33974 Episode Num: 52 Reward: -2.056700224747974\n",
            "Total Timesteps: 33994 Episode Num: 53 Reward: -1.5255706977255388\n",
            "Total Timesteps: 34014 Episode Num: 54 Reward: -2.6477857107726908\n",
            "Total Timesteps: 34034 Episode Num: 55 Reward: -0.6592099078898324\n",
            "Total Timesteps: 34054 Episode Num: 56 Reward: -0.9205734910264898\n",
            "Total Timesteps: 34074 Episode Num: 57 Reward: -1.510744709202903\n",
            "Total Timesteps: 34094 Episode Num: 58 Reward: -1.3125005633205022\n",
            "Total Timesteps: 34114 Episode Num: 59 Reward: -2.436738771974415\n",
            "Total Timesteps: 34134 Episode Num: 60 Reward: -1.5562975758618443\n",
            "Total Timesteps: 34154 Episode Num: 61 Reward: -2.319418439483386\n",
            "Total Timesteps: 34174 Episode Num: 62 Reward: -1.8014787680396762\n",
            "Total Timesteps: 34194 Episode Num: 63 Reward: -2.0152094772100555\n",
            "Total Timesteps: 34214 Episode Num: 64 Reward: -2.321239262301992\n",
            "Total Timesteps: 34234 Episode Num: 65 Reward: -1.3213211954934132\n",
            "Total Timesteps: 35234 Episode Num: 66 Reward: 392.8129199778169\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 397.700135\n",
            "---------------------------------------\n",
            "Total Timesteps: 36234 Episode Num: 67 Reward: 340.32351627790155\n",
            "Total Timesteps: 36398 Episode Num: 68 Reward: 68.9028545731389\n",
            "Total Timesteps: 37398 Episode Num: 69 Reward: 627.8422092145084\n",
            "Total Timesteps: 38398 Episode Num: 70 Reward: 345.20064642441787\n",
            "Total Timesteps: 39398 Episode Num: 71 Reward: 114.64611112411876\n",
            "Total Timesteps: 40398 Episode Num: 72 Reward: 390.85008482183827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 366.229896\n",
            "---------------------------------------\n",
            "Total Timesteps: 41398 Episode Num: 73 Reward: 336.75673779281254\n",
            "Total Timesteps: 42398 Episode Num: 74 Reward: 307.4949849422101\n",
            "Total Timesteps: 43398 Episode Num: 75 Reward: 449.2741879671333\n",
            "Total Timesteps: 44398 Episode Num: 76 Reward: 321.72878129791457\n",
            "Total Timesteps: 45398 Episode Num: 77 Reward: 581.4897835419439\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 441.062761\n",
            "---------------------------------------\n",
            "Total Timesteps: 46398 Episode Num: 78 Reward: 545.1603412580226\n",
            "Total Timesteps: 47398 Episode Num: 79 Reward: 375.00508199641615\n",
            "Total Timesteps: 48398 Episode Num: 80 Reward: 370.50307787913425\n",
            "Total Timesteps: 49398 Episode Num: 81 Reward: 285.80674308558633\n",
            "Total Timesteps: 50398 Episode Num: 82 Reward: 411.1774016047757\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 309.270975\n",
            "---------------------------------------\n",
            "Total Timesteps: 51398 Episode Num: 83 Reward: 421.2688591426128\n",
            "Total Timesteps: 52398 Episode Num: 84 Reward: 653.6587110289387\n",
            "Total Timesteps: 52441 Episode Num: 85 Reward: 7.568610140747607\n",
            "Total Timesteps: 52486 Episode Num: 86 Reward: 5.258147203757943\n",
            "Total Timesteps: 53486 Episode Num: 87 Reward: 361.39175622075766\n",
            "Total Timesteps: 53574 Episode Num: 88 Reward: 31.446608509430735\n",
            "Total Timesteps: 53779 Episode Num: 89 Reward: 74.95877937733415\n",
            "Total Timesteps: 54001 Episode Num: 90 Reward: 71.69444919607389\n",
            "Total Timesteps: 55001 Episode Num: 91 Reward: 140.54264278009455\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 169.918307\n",
            "---------------------------------------\n",
            "Total Timesteps: 56001 Episode Num: 92 Reward: 159.95563801146557\n",
            "Total Timesteps: 57001 Episode Num: 93 Reward: 179.03908010984685\n",
            "Total Timesteps: 58001 Episode Num: 94 Reward: 205.3320380318986\n",
            "Total Timesteps: 59001 Episode Num: 95 Reward: 109.51616227235675\n",
            "Total Timesteps: 60001 Episode Num: 96 Reward: 186.84194661334516\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 153.801936\n",
            "---------------------------------------\n",
            "Total Timesteps: 61001 Episode Num: 97 Reward: 201.40631563308503\n",
            "Total Timesteps: 62001 Episode Num: 98 Reward: 515.8816727494592\n",
            "Total Timesteps: 62197 Episode Num: 99 Reward: 19.417139473254046\n",
            "Total Timesteps: 63197 Episode Num: 100 Reward: 193.17384779076517\n",
            "Total Timesteps: 64197 Episode Num: 101 Reward: 456.9734225768544\n",
            "Total Timesteps: 65197 Episode Num: 102 Reward: 535.2540374114186\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 488.530834\n",
            "---------------------------------------\n",
            "Total Timesteps: 66197 Episode Num: 103 Reward: 415.5931476233741\n",
            "Total Timesteps: 67197 Episode Num: 104 Reward: 697.5396092700494\n",
            "Total Timesteps: 68197 Episode Num: 105 Reward: 667.8946299565285\n",
            "Total Timesteps: 69197 Episode Num: 106 Reward: 611.1224250962787\n",
            "Total Timesteps: 70197 Episode Num: 107 Reward: 335.5681106654576\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 358.980221\n",
            "---------------------------------------\n",
            "Total Timesteps: 71197 Episode Num: 108 Reward: 445.70408618596514\n",
            "Total Timesteps: 72197 Episode Num: 109 Reward: 575.2338044857587\n",
            "Total Timesteps: 73197 Episode Num: 110 Reward: 422.90535942480506\n",
            "Total Timesteps: 74197 Episode Num: 111 Reward: 158.16897171841848\n",
            "Total Timesteps: 75197 Episode Num: 112 Reward: 454.13721589778146\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 487.630260\n",
            "---------------------------------------\n",
            "Total Timesteps: 76197 Episode Num: 113 Reward: 320.5017252709552\n",
            "Total Timesteps: 77197 Episode Num: 114 Reward: 283.1571639946479\n",
            "Total Timesteps: 78197 Episode Num: 115 Reward: 548.1242815282033\n",
            "Total Timesteps: 79197 Episode Num: 116 Reward: 274.44114064438946\n",
            "Total Timesteps: 79793 Episode Num: 117 Reward: 297.02096843216134\n",
            "Total Timesteps: 80793 Episode Num: 118 Reward: 561.9952621054643\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 590.965639\n",
            "---------------------------------------\n",
            "Total Timesteps: 81793 Episode Num: 119 Reward: 632.5534318425651\n",
            "Total Timesteps: 82793 Episode Num: 120 Reward: 525.1429704768075\n",
            "Total Timesteps: 83793 Episode Num: 121 Reward: 267.10103266990956\n",
            "Total Timesteps: 84262 Episode Num: 122 Reward: 225.59218036786058\n",
            "Total Timesteps: 85262 Episode Num: 123 Reward: 533.6755236774602\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 503.610246\n",
            "---------------------------------------\n",
            "Total Timesteps: 86262 Episode Num: 124 Reward: 511.86639290899893\n",
            "Total Timesteps: 87262 Episode Num: 125 Reward: 543.6323954945495\n",
            "Total Timesteps: 88262 Episode Num: 126 Reward: 327.57646889144627\n",
            "Total Timesteps: 89262 Episode Num: 127 Reward: 236.30815702998666\n",
            "Total Timesteps: 90262 Episode Num: 128 Reward: 339.66158215792905\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 592.776299\n",
            "---------------------------------------\n",
            "Total Timesteps: 91262 Episode Num: 129 Reward: 635.3747406764842\n",
            "Total Timesteps: 92262 Episode Num: 130 Reward: 363.92345453135044\n",
            "Total Timesteps: 93262 Episode Num: 131 Reward: 423.5922995214269\n",
            "Total Timesteps: 94262 Episode Num: 132 Reward: 330.8321276774413\n",
            "Total Timesteps: 95262 Episode Num: 133 Reward: 460.48132391356125\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 458.897578\n",
            "---------------------------------------\n",
            "Total Timesteps: 96262 Episode Num: 134 Reward: 452.52699535288633\n",
            "Total Timesteps: 97262 Episode Num: 135 Reward: 371.571849278287\n",
            "Total Timesteps: 97289 Episode Num: 136 Reward: 4.661696331482534\n",
            "Total Timesteps: 98289 Episode Num: 137 Reward: 477.40503663401967\n",
            "Total Timesteps: 99289 Episode Num: 138 Reward: 606.3364967813742\n",
            "Total Timesteps: 99980 Episode Num: 139 Reward: 403.2950468775336\n",
            "Total Timesteps: 100980 Episode Num: 140 Reward: 611.9676945009312\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 537.594511\n",
            "---------------------------------------\n",
            "Total Timesteps: 101980 Episode Num: 141 Reward: 593.968362882925\n",
            "Total Timesteps: 102980 Episode Num: 142 Reward: 345.21588563235554\n",
            "Total Timesteps: 103980 Episode Num: 143 Reward: 337.26947880356647\n",
            "Total Timesteps: 104980 Episode Num: 144 Reward: 658.2322587675649\n",
            "Total Timesteps: 105980 Episode Num: 145 Reward: 630.9872347529337\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 527.648880\n",
            "---------------------------------------\n",
            "Total Timesteps: 106980 Episode Num: 146 Reward: 708.5577319738833\n",
            "Total Timesteps: 107980 Episode Num: 147 Reward: 590.2714489649429\n",
            "Total Timesteps: 108980 Episode Num: 148 Reward: 794.6073620017282\n",
            "Total Timesteps: 109001 Episode Num: 149 Reward: 2.0941802481736\n",
            "Total Timesteps: 109022 Episode Num: 150 Reward: 1.787023929049167\n",
            "Total Timesteps: 110022 Episode Num: 151 Reward: 506.6032105702443\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 637.891195\n",
            "---------------------------------------\n",
            "Total Timesteps: 111022 Episode Num: 152 Reward: 673.7404536440896\n",
            "Total Timesteps: 112022 Episode Num: 153 Reward: 674.5928507687463\n",
            "Total Timesteps: 113022 Episode Num: 154 Reward: 722.839860024322\n",
            "Total Timesteps: 114022 Episode Num: 155 Reward: 652.1581564247095\n",
            "Total Timesteps: 115022 Episode Num: 156 Reward: 481.5538110660605\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 596.589887\n",
            "---------------------------------------\n",
            "Total Timesteps: 116022 Episode Num: 157 Reward: 701.5305140310828\n",
            "Total Timesteps: 117022 Episode Num: 158 Reward: 607.9477124860731\n",
            "Total Timesteps: 118022 Episode Num: 159 Reward: 332.1953670889646\n",
            "Total Timesteps: 119022 Episode Num: 160 Reward: 380.43775001348547\n",
            "Total Timesteps: 120022 Episode Num: 161 Reward: 524.1661871125258\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 619.366679\n",
            "---------------------------------------\n",
            "Total Timesteps: 121022 Episode Num: 162 Reward: 590.8520870902166\n",
            "Total Timesteps: 122022 Episode Num: 163 Reward: 464.952014880484\n",
            "Total Timesteps: 123022 Episode Num: 164 Reward: 609.630024697027\n",
            "Total Timesteps: 124022 Episode Num: 165 Reward: 564.1086100951673\n",
            "Total Timesteps: 125022 Episode Num: 166 Reward: 710.48828681478\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 557.145831\n",
            "---------------------------------------\n",
            "Total Timesteps: 126022 Episode Num: 167 Reward: 463.64979469135733\n",
            "Total Timesteps: 127022 Episode Num: 168 Reward: 645.2819647795886\n",
            "Total Timesteps: 128022 Episode Num: 169 Reward: 497.431167371657\n",
            "Total Timesteps: 129022 Episode Num: 170 Reward: 668.8034563515233\n",
            "Total Timesteps: 130022 Episode Num: 171 Reward: 504.08532299904886\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 669.806443\n",
            "---------------------------------------\n",
            "Total Timesteps: 131022 Episode Num: 172 Reward: 650.1986217990062\n",
            "Total Timesteps: 132022 Episode Num: 173 Reward: 402.57119952462443\n",
            "Total Timesteps: 133022 Episode Num: 174 Reward: 340.6223584917241\n",
            "Total Timesteps: 134022 Episode Num: 175 Reward: 495.8269343202138\n",
            "Total Timesteps: 135022 Episode Num: 176 Reward: 722.4771902818953\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 577.864218\n",
            "---------------------------------------\n",
            "Total Timesteps: 136022 Episode Num: 177 Reward: 753.1828172060663\n",
            "Total Timesteps: 137022 Episode Num: 178 Reward: 691.2059207148095\n",
            "Total Timesteps: 138022 Episode Num: 179 Reward: 534.8654689700447\n",
            "Total Timesteps: 139022 Episode Num: 180 Reward: 617.1027993372334\n",
            "Total Timesteps: 140022 Episode Num: 181 Reward: 426.36139411518604\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 608.810674\n",
            "---------------------------------------\n",
            "Total Timesteps: 141022 Episode Num: 182 Reward: 660.9611764560267\n",
            "Total Timesteps: 142022 Episode Num: 183 Reward: 787.7990344068302\n",
            "Total Timesteps: 143022 Episode Num: 184 Reward: 591.8808572272734\n",
            "Total Timesteps: 144022 Episode Num: 185 Reward: 354.433021106325\n",
            "Total Timesteps: 145022 Episode Num: 186 Reward: 477.49614254760303\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 562.925724\n",
            "---------------------------------------\n",
            "Total Timesteps: 146022 Episode Num: 187 Reward: 412.7673263445847\n",
            "Total Timesteps: 147022 Episode Num: 188 Reward: 519.3539861788718\n",
            "Total Timesteps: 148022 Episode Num: 189 Reward: 672.7121653705191\n",
            "Total Timesteps: 149022 Episode Num: 190 Reward: 546.0968017625224\n",
            "Total Timesteps: 150022 Episode Num: 191 Reward: 777.8482704894471\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 575.826245\n",
            "---------------------------------------\n",
            "Total Timesteps: 151022 Episode Num: 192 Reward: 553.4913327702526\n",
            "Total Timesteps: 151216 Episode Num: 193 Reward: 112.52822670380665\n",
            "Total Timesteps: 152216 Episode Num: 194 Reward: 522.8715943192594\n",
            "Total Timesteps: 153216 Episode Num: 195 Reward: 668.1285370752032\n",
            "Total Timesteps: 154216 Episode Num: 196 Reward: 576.0375174111657\n",
            "Total Timesteps: 155216 Episode Num: 197 Reward: 692.0559608232534\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 618.920075\n",
            "---------------------------------------\n",
            "Total Timesteps: 156216 Episode Num: 198 Reward: 740.3877317281092\n",
            "Total Timesteps: 157216 Episode Num: 199 Reward: 752.5017500917047\n",
            "Total Timesteps: 158216 Episode Num: 200 Reward: 519.127821193283\n",
            "Total Timesteps: 159216 Episode Num: 201 Reward: 682.4078999109745\n",
            "Total Timesteps: 160216 Episode Num: 202 Reward: 568.1707712000164\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 642.582130\n",
            "---------------------------------------\n",
            "Total Timesteps: 161216 Episode Num: 203 Reward: 746.5344171582717\n",
            "Total Timesteps: 162216 Episode Num: 204 Reward: 649.7246694115853\n",
            "Total Timesteps: 163216 Episode Num: 205 Reward: 733.4285378836693\n",
            "Total Timesteps: 164216 Episode Num: 206 Reward: 642.0100071310126\n",
            "Total Timesteps: 165216 Episode Num: 207 Reward: 714.583823195551\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 682.008213\n",
            "---------------------------------------\n",
            "Total Timesteps: 166216 Episode Num: 208 Reward: 746.7989758614275\n",
            "Total Timesteps: 167216 Episode Num: 209 Reward: 742.3623354047148\n",
            "Total Timesteps: 168216 Episode Num: 210 Reward: 726.2885302701989\n",
            "Total Timesteps: 169216 Episode Num: 211 Reward: 734.9607874686232\n",
            "Total Timesteps: 170216 Episode Num: 212 Reward: 532.9248401164383\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 705.372877\n",
            "---------------------------------------\n",
            "Total Timesteps: 171216 Episode Num: 213 Reward: 627.5433193734282\n",
            "Total Timesteps: 172216 Episode Num: 214 Reward: 647.6996224670861\n",
            "Total Timesteps: 173216 Episode Num: 215 Reward: 827.4573432233963\n",
            "Total Timesteps: 174216 Episode Num: 216 Reward: 705.0746037540988\n",
            "Total Timesteps: 175216 Episode Num: 217 Reward: 737.4469965874872\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 669.321334\n",
            "---------------------------------------\n",
            "Total Timesteps: 176216 Episode Num: 218 Reward: 750.9178471947286\n",
            "Total Timesteps: 177216 Episode Num: 219 Reward: 768.2621674822608\n",
            "Total Timesteps: 178216 Episode Num: 220 Reward: 734.6386814228296\n",
            "Total Timesteps: 179216 Episode Num: 221 Reward: 316.24658040978636\n",
            "Total Timesteps: 180216 Episode Num: 222 Reward: 688.001536994065\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 705.193719\n",
            "---------------------------------------\n",
            "Total Timesteps: 181216 Episode Num: 223 Reward: 735.7076151505894\n",
            "Total Timesteps: 182216 Episode Num: 224 Reward: 759.6017366889779\n",
            "Total Timesteps: 183216 Episode Num: 225 Reward: 710.732388424774\n",
            "Total Timesteps: 184216 Episode Num: 226 Reward: 829.1384481295722\n",
            "Total Timesteps: 185216 Episode Num: 227 Reward: 393.2382403767095\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 682.929453\n",
            "---------------------------------------\n",
            "Total Timesteps: 186216 Episode Num: 228 Reward: 602.6875028275358\n",
            "Total Timesteps: 187216 Episode Num: 229 Reward: 543.87782956402\n",
            "Total Timesteps: 188216 Episode Num: 230 Reward: 605.9510508254846\n",
            "Total Timesteps: 189216 Episode Num: 231 Reward: 681.926344222339\n",
            "Total Timesteps: 190216 Episode Num: 232 Reward: 753.4256431821814\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 657.544215\n",
            "---------------------------------------\n",
            "Total Timesteps: 191216 Episode Num: 233 Reward: 641.502444800972\n",
            "Total Timesteps: 192216 Episode Num: 234 Reward: 726.5922006332322\n",
            "Total Timesteps: 193216 Episode Num: 235 Reward: 624.4388268638072\n",
            "Total Timesteps: 194216 Episode Num: 236 Reward: 641.3191526679602\n",
            "Total Timesteps: 195216 Episode Num: 237 Reward: 625.6831541191084\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 682.466922\n",
            "---------------------------------------\n",
            "Total Timesteps: 196216 Episode Num: 238 Reward: 654.6231598186329\n",
            "Total Timesteps: 197216 Episode Num: 239 Reward: 657.4401370416829\n",
            "Total Timesteps: 198216 Episode Num: 240 Reward: 839.5605004999254\n",
            "Total Timesteps: 199216 Episode Num: 241 Reward: 672.0284438624204\n",
            "Total Timesteps: 200216 Episode Num: 242 Reward: 734.6441703507959\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 720.908317\n",
            "---------------------------------------\n",
            "Total Timesteps: 201216 Episode Num: 243 Reward: 765.9618219245531\n",
            "Total Timesteps: 202216 Episode Num: 244 Reward: 712.9848444745431\n",
            "Total Timesteps: 203216 Episode Num: 245 Reward: 812.7663376624818\n",
            "Total Timesteps: 204216 Episode Num: 246 Reward: 768.7427294336646\n",
            "Total Timesteps: 205216 Episode Num: 247 Reward: 750.3558753995131\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 846.594567\n",
            "---------------------------------------\n",
            "Total Timesteps: 206216 Episode Num: 248 Reward: 783.2129033435584\n",
            "Total Timesteps: 207216 Episode Num: 249 Reward: 691.9545685452201\n",
            "Total Timesteps: 208216 Episode Num: 250 Reward: 676.7149972870806\n",
            "Total Timesteps: 209216 Episode Num: 251 Reward: 777.8139716580623\n",
            "Total Timesteps: 210216 Episode Num: 252 Reward: 768.9811272411517\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 849.834163\n",
            "---------------------------------------\n",
            "Total Timesteps: 211216 Episode Num: 253 Reward: 734.7483895016456\n",
            "Total Timesteps: 212216 Episode Num: 254 Reward: 674.1610822394044\n",
            "Total Timesteps: 213216 Episode Num: 255 Reward: 763.956722008491\n",
            "Total Timesteps: 214216 Episode Num: 256 Reward: 682.62749211503\n",
            "Total Timesteps: 215216 Episode Num: 257 Reward: 767.0641897746636\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 787.054234\n",
            "---------------------------------------\n",
            "Total Timesteps: 216216 Episode Num: 258 Reward: 849.2858023432195\n",
            "Total Timesteps: 217216 Episode Num: 259 Reward: 871.077668101432\n",
            "Total Timesteps: 218216 Episode Num: 260 Reward: 727.6945568320621\n",
            "Total Timesteps: 219216 Episode Num: 261 Reward: 650.846459049992\n",
            "Total Timesteps: 220216 Episode Num: 262 Reward: 688.8453795355896\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 714.643676\n",
            "---------------------------------------\n",
            "Total Timesteps: 221216 Episode Num: 263 Reward: 664.7527047315381\n",
            "Total Timesteps: 222216 Episode Num: 264 Reward: 885.076346507306\n",
            "Total Timesteps: 223216 Episode Num: 265 Reward: 451.7846730742427\n",
            "Total Timesteps: 224216 Episode Num: 266 Reward: 1023.5424403778268\n",
            "Total Timesteps: 225216 Episode Num: 267 Reward: 873.4015800786262\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 840.387251\n",
            "---------------------------------------\n",
            "Total Timesteps: 226216 Episode Num: 268 Reward: 944.525074360647\n",
            "Total Timesteps: 227216 Episode Num: 269 Reward: 892.3182379536058\n",
            "Total Timesteps: 228216 Episode Num: 270 Reward: 798.1084710502731\n",
            "Total Timesteps: 229216 Episode Num: 271 Reward: 893.4752724707433\n",
            "Total Timesteps: 230216 Episode Num: 272 Reward: 835.3569478814575\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 859.120418\n",
            "---------------------------------------\n",
            "Total Timesteps: 231216 Episode Num: 273 Reward: 982.4489083721924\n",
            "Total Timesteps: 232216 Episode Num: 274 Reward: 829.270184662436\n",
            "Total Timesteps: 233216 Episode Num: 275 Reward: 887.0283696250068\n",
            "Total Timesteps: 234216 Episode Num: 276 Reward: 907.8324301097671\n",
            "Total Timesteps: 235216 Episode Num: 277 Reward: 866.5616706981648\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 699.829928\n",
            "---------------------------------------\n",
            "Total Timesteps: 236216 Episode Num: 278 Reward: 861.0952264386428\n",
            "Total Timesteps: 237216 Episode Num: 279 Reward: 655.9396469275372\n",
            "Total Timesteps: 238216 Episode Num: 280 Reward: 873.6889565526377\n",
            "Total Timesteps: 239216 Episode Num: 281 Reward: 780.9688861832077\n",
            "Total Timesteps: 240216 Episode Num: 282 Reward: 799.7213428692432\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 728.810775\n",
            "---------------------------------------\n",
            "Total Timesteps: 241216 Episode Num: 283 Reward: 631.5649830875456\n",
            "Total Timesteps: 242216 Episode Num: 284 Reward: 967.5275741471701\n",
            "Total Timesteps: 243216 Episode Num: 285 Reward: 695.4473350335188\n",
            "Total Timesteps: 244216 Episode Num: 286 Reward: 736.8480494902225\n",
            "Total Timesteps: 245216 Episode Num: 287 Reward: 755.5288944902744\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 785.198255\n",
            "---------------------------------------\n",
            "Total Timesteps: 246216 Episode Num: 288 Reward: 886.8717179341443\n",
            "Total Timesteps: 247216 Episode Num: 289 Reward: 729.6083147040333\n",
            "Total Timesteps: 248216 Episode Num: 290 Reward: 903.0078164050803\n",
            "Total Timesteps: 249216 Episode Num: 291 Reward: 733.1694647070558\n",
            "Total Timesteps: 250216 Episode Num: 292 Reward: 804.5086743501057\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 813.810616\n",
            "---------------------------------------\n",
            "Total Timesteps: 251216 Episode Num: 293 Reward: 861.0500232048443\n",
            "Total Timesteps: 252216 Episode Num: 294 Reward: 503.061799173526\n",
            "Total Timesteps: 253216 Episode Num: 295 Reward: 931.6243828785073\n",
            "Total Timesteps: 254216 Episode Num: 296 Reward: 746.442994554256\n",
            "Total Timesteps: 255216 Episode Num: 297 Reward: 1126.8270575019662\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 935.685517\n",
            "---------------------------------------\n",
            "Total Timesteps: 256216 Episode Num: 298 Reward: 716.4034572614992\n",
            "Total Timesteps: 257216 Episode Num: 299 Reward: 654.5326162800188\n",
            "Total Timesteps: 258216 Episode Num: 300 Reward: 849.3152772315295\n",
            "Total Timesteps: 259216 Episode Num: 301 Reward: 894.2454314319884\n",
            "Total Timesteps: 260216 Episode Num: 302 Reward: 1016.7082171969497\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1227.346957\n",
            "---------------------------------------\n",
            "Total Timesteps: 261216 Episode Num: 303 Reward: 1278.795000610932\n",
            "Total Timesteps: 262216 Episode Num: 304 Reward: 1230.4187490050917\n",
            "Total Timesteps: 263216 Episode Num: 305 Reward: 723.3536961803572\n",
            "Total Timesteps: 264216 Episode Num: 306 Reward: 1047.622739063257\n",
            "Total Timesteps: 265216 Episode Num: 307 Reward: 1093.2249956411242\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1209.577387\n",
            "---------------------------------------\n",
            "Total Timesteps: 266216 Episode Num: 308 Reward: 1130.5881553724428\n",
            "Total Timesteps: 267216 Episode Num: 309 Reward: 957.5331860532141\n",
            "Total Timesteps: 268216 Episode Num: 310 Reward: 1173.086660565449\n",
            "Total Timesteps: 269216 Episode Num: 311 Reward: 1290.5914149697053\n",
            "Total Timesteps: 270216 Episode Num: 312 Reward: 1263.3196089069115\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 833.710910\n",
            "---------------------------------------\n",
            "Total Timesteps: 271216 Episode Num: 313 Reward: 798.0331586442919\n",
            "Total Timesteps: 272216 Episode Num: 314 Reward: 1306.6842602868091\n",
            "Total Timesteps: 273216 Episode Num: 315 Reward: 1353.3643563461962\n",
            "Total Timesteps: 274216 Episode Num: 316 Reward: 754.8702533669218\n",
            "Total Timesteps: 275216 Episode Num: 317 Reward: 1163.1723110833875\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1197.689870\n",
            "---------------------------------------\n",
            "Total Timesteps: 276216 Episode Num: 318 Reward: 635.0855293924222\n",
            "Total Timesteps: 277216 Episode Num: 319 Reward: 1341.9617410229323\n",
            "Total Timesteps: 278216 Episode Num: 320 Reward: 1323.187850071643\n",
            "Total Timesteps: 279216 Episode Num: 321 Reward: 1020.9176624045886\n",
            "Total Timesteps: 280216 Episode Num: 322 Reward: 761.4378259559909\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1300.444259\n",
            "---------------------------------------\n",
            "Total Timesteps: 281216 Episode Num: 323 Reward: 940.1057685638458\n",
            "Total Timesteps: 282216 Episode Num: 324 Reward: 1418.2336364998728\n",
            "Total Timesteps: 283216 Episode Num: 325 Reward: 1496.8744730323763\n",
            "Total Timesteps: 284216 Episode Num: 326 Reward: 1435.3982202338166\n",
            "Total Timesteps: 285216 Episode Num: 327 Reward: 1359.6355796943715\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1056.159216\n",
            "---------------------------------------\n",
            "Total Timesteps: 286216 Episode Num: 328 Reward: 1329.6849259883243\n",
            "Total Timesteps: 287216 Episode Num: 329 Reward: 853.3911889028135\n",
            "Total Timesteps: 288216 Episode Num: 330 Reward: 1110.5887154717855\n",
            "Total Timesteps: 289216 Episode Num: 331 Reward: 1348.0773129595477\n",
            "Total Timesteps: 290216 Episode Num: 332 Reward: 1542.9489215843641\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1260.285616\n",
            "---------------------------------------\n",
            "Total Timesteps: 291216 Episode Num: 333 Reward: 1619.2409074870652\n",
            "Total Timesteps: 292216 Episode Num: 334 Reward: 1379.898066841488\n",
            "Total Timesteps: 293216 Episode Num: 335 Reward: 1196.5062746930623\n",
            "Total Timesteps: 294216 Episode Num: 336 Reward: 904.9678903982627\n",
            "Total Timesteps: 295216 Episode Num: 337 Reward: 948.3748982397656\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1318.073960\n",
            "---------------------------------------\n",
            "Total Timesteps: 296216 Episode Num: 338 Reward: 1587.9493410816033\n",
            "Total Timesteps: 297216 Episode Num: 339 Reward: 739.0285898225761\n",
            "Total Timesteps: 298216 Episode Num: 340 Reward: 1400.6643871260087\n",
            "Total Timesteps: 299216 Episode Num: 341 Reward: 1209.484139076578\n",
            "Total Timesteps: 300216 Episode Num: 342 Reward: 1154.1847298723164\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 960.588319\n",
            "---------------------------------------\n",
            "Total Timesteps: 301216 Episode Num: 343 Reward: 1043.920278744324\n",
            "Total Timesteps: 302216 Episode Num: 344 Reward: 1032.9177664686501\n",
            "Total Timesteps: 303216 Episode Num: 345 Reward: 1523.0798867088097\n",
            "Total Timesteps: 304216 Episode Num: 346 Reward: 1430.1771381884\n",
            "Total Timesteps: 305216 Episode Num: 347 Reward: 1153.2784137780147\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1522.736309\n",
            "---------------------------------------\n",
            "Total Timesteps: 306216 Episode Num: 348 Reward: 1602.5144790546196\n",
            "Total Timesteps: 307216 Episode Num: 349 Reward: 1312.6666498253728\n",
            "Total Timesteps: 308216 Episode Num: 350 Reward: 725.1943906364984\n",
            "Total Timesteps: 309216 Episode Num: 351 Reward: 1318.854204454891\n",
            "Total Timesteps: 310216 Episode Num: 352 Reward: 1322.3171061033208\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1824.132015\n",
            "---------------------------------------\n",
            "Total Timesteps: 311216 Episode Num: 353 Reward: 1777.7556239962664\n",
            "Total Timesteps: 312216 Episode Num: 354 Reward: 1341.2480371106924\n",
            "Total Timesteps: 313216 Episode Num: 355 Reward: 1513.904258028361\n",
            "Total Timesteps: 314216 Episode Num: 356 Reward: 1300.6925332156072\n",
            "Total Timesteps: 315216 Episode Num: 357 Reward: 1767.3343832494295\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1735.102716\n",
            "---------------------------------------\n",
            "Total Timesteps: 316216 Episode Num: 358 Reward: 1658.170702449612\n",
            "Total Timesteps: 317216 Episode Num: 359 Reward: 1612.8320856252374\n",
            "Total Timesteps: 318216 Episode Num: 360 Reward: 1704.6157953582228\n",
            "Total Timesteps: 319216 Episode Num: 361 Reward: 1736.7544359858052\n",
            "Total Timesteps: 320216 Episode Num: 362 Reward: 1221.0027565393784\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1919.281337\n",
            "---------------------------------------\n",
            "Total Timesteps: 321216 Episode Num: 363 Reward: 1896.1215852295531\n",
            "Total Timesteps: 322216 Episode Num: 364 Reward: 1741.3464463247392\n",
            "Total Timesteps: 323216 Episode Num: 365 Reward: 1944.7312385596358\n",
            "Total Timesteps: 324216 Episode Num: 366 Reward: 1813.8593641097052\n",
            "Total Timesteps: 325216 Episode Num: 367 Reward: 1848.2985364994388\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2003.006279\n",
            "---------------------------------------\n",
            "Total Timesteps: 326216 Episode Num: 368 Reward: 1931.3869507374418\n",
            "Total Timesteps: 327216 Episode Num: 369 Reward: 1969.6978107025022\n",
            "Total Timesteps: 328216 Episode Num: 370 Reward: 2009.7025009239615\n",
            "Total Timesteps: 329216 Episode Num: 371 Reward: 1835.2427359790286\n",
            "Total Timesteps: 330216 Episode Num: 372 Reward: 1199.917796190292\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1769.097600\n",
            "---------------------------------------\n",
            "Total Timesteps: 331216 Episode Num: 373 Reward: 1754.6392821803536\n",
            "Total Timesteps: 332216 Episode Num: 374 Reward: 1732.1462677664836\n",
            "Total Timesteps: 333216 Episode Num: 375 Reward: 2028.8408011711247\n",
            "Total Timesteps: 334216 Episode Num: 376 Reward: 1844.165671497464\n",
            "Total Timesteps: 335216 Episode Num: 377 Reward: 1889.1327827374828\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1589.164958\n",
            "---------------------------------------\n",
            "Total Timesteps: 336216 Episode Num: 378 Reward: 1656.8952788738732\n",
            "Total Timesteps: 337216 Episode Num: 379 Reward: 1788.1572465844538\n",
            "Total Timesteps: 338216 Episode Num: 380 Reward: 1823.406409700178\n",
            "Total Timesteps: 339216 Episode Num: 381 Reward: 1861.2518138492374\n",
            "Total Timesteps: 340216 Episode Num: 382 Reward: 1828.5228126820828\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1970.555646\n",
            "---------------------------------------\n",
            "Total Timesteps: 341216 Episode Num: 383 Reward: 1963.491303828454\n",
            "Total Timesteps: 342216 Episode Num: 384 Reward: 1723.653199689869\n",
            "Total Timesteps: 343216 Episode Num: 385 Reward: 2007.7719685123627\n",
            "Total Timesteps: 344216 Episode Num: 386 Reward: 2041.436902175495\n",
            "Total Timesteps: 345216 Episode Num: 387 Reward: 1973.2278830341838\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2209.436810\n",
            "---------------------------------------\n",
            "Total Timesteps: 346216 Episode Num: 388 Reward: 2209.4458186025945\n",
            "Total Timesteps: 347216 Episode Num: 389 Reward: 2104.5822129388102\n",
            "Total Timesteps: 348216 Episode Num: 390 Reward: 1982.6016957512136\n",
            "Total Timesteps: 349216 Episode Num: 391 Reward: 2140.977531684396\n",
            "Total Timesteps: 350216 Episode Num: 392 Reward: 1990.488571654955\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2119.519156\n",
            "---------------------------------------\n",
            "Total Timesteps: 351216 Episode Num: 393 Reward: 2097.092719156101\n",
            "Total Timesteps: 352216 Episode Num: 394 Reward: 1943.9855562534417\n",
            "Total Timesteps: 353216 Episode Num: 395 Reward: 2181.2963601134434\n",
            "Total Timesteps: 354216 Episode Num: 396 Reward: 1998.0586937878318\n",
            "Total Timesteps: 355216 Episode Num: 397 Reward: 2142.2256588676723\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2253.726135\n",
            "---------------------------------------\n",
            "Total Timesteps: 356216 Episode Num: 398 Reward: 2255.3015395995317\n",
            "Total Timesteps: 357216 Episode Num: 399 Reward: 2249.384404262713\n",
            "Total Timesteps: 358216 Episode Num: 400 Reward: 2235.4631032409784\n",
            "Total Timesteps: 359216 Episode Num: 401 Reward: 2187.3241938371216\n",
            "Total Timesteps: 360216 Episode Num: 402 Reward: 2084.169841137298\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2124.266746\n",
            "---------------------------------------\n",
            "Total Timesteps: 361216 Episode Num: 403 Reward: 2121.6969957710085\n",
            "Total Timesteps: 362216 Episode Num: 404 Reward: 2155.645666782312\n",
            "Total Timesteps: 363216 Episode Num: 405 Reward: 2143.9627344306955\n",
            "Total Timesteps: 364216 Episode Num: 406 Reward: 2056.1239453725802\n",
            "Total Timesteps: 365216 Episode Num: 407 Reward: 2132.283942987921\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2234.634222\n",
            "---------------------------------------\n",
            "Total Timesteps: 366216 Episode Num: 408 Reward: 2229.5732401734626\n",
            "Total Timesteps: 367216 Episode Num: 409 Reward: 2144.110901977746\n",
            "Total Timesteps: 368216 Episode Num: 410 Reward: 2185.0385004322375\n",
            "Total Timesteps: 369216 Episode Num: 411 Reward: 2252.6340303375987\n",
            "Total Timesteps: 370216 Episode Num: 412 Reward: 2249.4855093486335\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2371.346754\n",
            "---------------------------------------\n",
            "Total Timesteps: 371216 Episode Num: 413 Reward: 2314.3035662203497\n",
            "Total Timesteps: 372216 Episode Num: 414 Reward: 2294.5802770219248\n",
            "Total Timesteps: 373216 Episode Num: 415 Reward: 2186.7493070343676\n",
            "Total Timesteps: 374216 Episode Num: 416 Reward: 2314.210327392417\n",
            "Total Timesteps: 375216 Episode Num: 417 Reward: 2221.4494653260467\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2277.225252\n",
            "---------------------------------------\n",
            "Total Timesteps: 376216 Episode Num: 418 Reward: 2251.938106394792\n",
            "Total Timesteps: 377216 Episode Num: 419 Reward: 2202.351801101312\n",
            "Total Timesteps: 378216 Episode Num: 420 Reward: 2293.269770601969\n",
            "Total Timesteps: 379216 Episode Num: 421 Reward: 2376.476072616505\n",
            "Total Timesteps: 380216 Episode Num: 422 Reward: 2302.277020051018\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2328.387606\n",
            "---------------------------------------\n",
            "Total Timesteps: 381216 Episode Num: 423 Reward: 2295.00631945247\n",
            "Total Timesteps: 382216 Episode Num: 424 Reward: 2261.724709106142\n",
            "Total Timesteps: 383216 Episode Num: 425 Reward: 2364.6497792016307\n",
            "Total Timesteps: 384216 Episode Num: 426 Reward: 2379.000222813538\n",
            "Total Timesteps: 385216 Episode Num: 427 Reward: 2377.163512739059\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2288.237437\n",
            "---------------------------------------\n",
            "Total Timesteps: 386216 Episode Num: 428 Reward: 2288.4773634241997\n",
            "Total Timesteps: 387216 Episode Num: 429 Reward: 2326.7636081241394\n",
            "Total Timesteps: 388216 Episode Num: 430 Reward: 2222.7294723989994\n",
            "Total Timesteps: 389216 Episode Num: 431 Reward: 2293.414769773814\n",
            "Total Timesteps: 390216 Episode Num: 432 Reward: 2344.4703976576347\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2265.987265\n",
            "---------------------------------------\n",
            "Total Timesteps: 391216 Episode Num: 433 Reward: 2322.327857069799\n",
            "Total Timesteps: 392216 Episode Num: 434 Reward: 2386.508656648386\n",
            "Total Timesteps: 393216 Episode Num: 435 Reward: 2320.5673450992017\n",
            "Total Timesteps: 394216 Episode Num: 436 Reward: 2336.7944367090527\n",
            "Total Timesteps: 395216 Episode Num: 437 Reward: 2269.6948591664745\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2375.678383\n",
            "---------------------------------------\n",
            "Total Timesteps: 396216 Episode Num: 438 Reward: 2375.895088298896\n",
            "Total Timesteps: 397216 Episode Num: 439 Reward: 2388.4933046115634\n",
            "Total Timesteps: 398216 Episode Num: 440 Reward: 2155.8410262378807\n",
            "Total Timesteps: 399216 Episode Num: 441 Reward: 2374.535254082376\n",
            "Total Timesteps: 400216 Episode Num: 442 Reward: 2332.276166186975\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2417.721413\n",
            "---------------------------------------\n",
            "Total Timesteps: 401216 Episode Num: 443 Reward: 2384.505022570293\n",
            "Total Timesteps: 402216 Episode Num: 444 Reward: 2286.9854526768745\n",
            "Total Timesteps: 403216 Episode Num: 445 Reward: 2340.553110147855\n",
            "Total Timesteps: 404216 Episode Num: 446 Reward: 2247.1774893141937\n",
            "Total Timesteps: 405216 Episode Num: 447 Reward: 2342.7657541139947\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2365.765771\n",
            "---------------------------------------\n",
            "Total Timesteps: 406216 Episode Num: 448 Reward: 2395.1951800822903\n",
            "Total Timesteps: 407216 Episode Num: 449 Reward: 2313.8162661909378\n",
            "Total Timesteps: 408216 Episode Num: 450 Reward: 2510.0734275124278\n",
            "Total Timesteps: 409216 Episode Num: 451 Reward: 2278.2860984440053\n",
            "Total Timesteps: 410216 Episode Num: 452 Reward: 2350.7833358635958\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2490.806644\n",
            "---------------------------------------\n",
            "Total Timesteps: 411216 Episode Num: 453 Reward: 2482.9737642254795\n",
            "Total Timesteps: 412216 Episode Num: 454 Reward: 2375.8680193428995\n",
            "Total Timesteps: 413216 Episode Num: 455 Reward: 2380.0272365590718\n",
            "Total Timesteps: 414216 Episode Num: 456 Reward: 2257.9472902006933\n",
            "Total Timesteps: 415216 Episode Num: 457 Reward: 2398.734027066195\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2369.058231\n",
            "---------------------------------------\n",
            "Total Timesteps: 416216 Episode Num: 458 Reward: 2390.2050069102124\n",
            "Total Timesteps: 417216 Episode Num: 459 Reward: 2458.6147768937635\n",
            "Total Timesteps: 418216 Episode Num: 460 Reward: 2391.594602032692\n",
            "Total Timesteps: 419216 Episode Num: 461 Reward: 2305.9453464425105\n",
            "Total Timesteps: 420216 Episode Num: 462 Reward: 2370.7588503308225\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2560.948502\n",
            "---------------------------------------\n",
            "Total Timesteps: 421216 Episode Num: 463 Reward: 2456.7559974492688\n",
            "Total Timesteps: 422216 Episode Num: 464 Reward: 2333.5615963849186\n",
            "Total Timesteps: 423216 Episode Num: 465 Reward: 2326.0715181986716\n",
            "Total Timesteps: 424216 Episode Num: 466 Reward: 2408.678360081891\n",
            "Total Timesteps: 425216 Episode Num: 467 Reward: 2396.1698433985002\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2427.752482\n",
            "---------------------------------------\n",
            "Total Timesteps: 426216 Episode Num: 468 Reward: 2440.593343820687\n",
            "Total Timesteps: 427216 Episode Num: 469 Reward: 2555.511708788466\n",
            "Total Timesteps: 428216 Episode Num: 470 Reward: 2526.712438938446\n",
            "Total Timesteps: 429216 Episode Num: 471 Reward: 2401.186714054933\n",
            "Total Timesteps: 430216 Episode Num: 472 Reward: 2553.1363073028015\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2429.995683\n",
            "---------------------------------------\n",
            "Total Timesteps: 431216 Episode Num: 473 Reward: 2419.4019773776545\n",
            "Total Timesteps: 432216 Episode Num: 474 Reward: 2614.963139534227\n",
            "Total Timesteps: 433216 Episode Num: 475 Reward: 2477.325432375032\n",
            "Total Timesteps: 434216 Episode Num: 476 Reward: 2561.7257190309897\n",
            "Total Timesteps: 435216 Episode Num: 477 Reward: 2523.0245793162144\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2512.467828\n",
            "---------------------------------------\n",
            "Total Timesteps: 436216 Episode Num: 478 Reward: 2472.0231538316716\n",
            "Total Timesteps: 437216 Episode Num: 479 Reward: 2529.0103708367924\n",
            "Total Timesteps: 438216 Episode Num: 480 Reward: 2590.31518572045\n",
            "Total Timesteps: 439216 Episode Num: 481 Reward: 2484.2687375402716\n",
            "Total Timesteps: 440216 Episode Num: 482 Reward: 2497.6911064896312\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2455.492145\n",
            "---------------------------------------\n",
            "Total Timesteps: 441216 Episode Num: 483 Reward: 2391.056905847796\n",
            "Total Timesteps: 442216 Episode Num: 484 Reward: 2571.805287833563\n",
            "Total Timesteps: 443216 Episode Num: 485 Reward: 2549.274723943513\n",
            "Total Timesteps: 444216 Episode Num: 486 Reward: 2574.882632008926\n",
            "Total Timesteps: 445216 Episode Num: 487 Reward: 2481.955370697273\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2543.855003\n",
            "---------------------------------------\n",
            "Total Timesteps: 446216 Episode Num: 488 Reward: 2507.790053126722\n",
            "Total Timesteps: 447216 Episode Num: 489 Reward: 2408.77437149223\n",
            "Total Timesteps: 448216 Episode Num: 490 Reward: 2341.508721950759\n",
            "Total Timesteps: 449216 Episode Num: 491 Reward: 2484.575719033939\n",
            "Total Timesteps: 450216 Episode Num: 492 Reward: 2498.4053474489315\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2576.691055\n",
            "---------------------------------------\n",
            "Total Timesteps: 451216 Episode Num: 493 Reward: 2558.9148760911276\n",
            "Total Timesteps: 452216 Episode Num: 494 Reward: 2528.1484079197567\n",
            "Total Timesteps: 453216 Episode Num: 495 Reward: 2425.9135459553104\n",
            "Total Timesteps: 454216 Episode Num: 496 Reward: 2463.0137876542567\n",
            "Total Timesteps: 455216 Episode Num: 497 Reward: 2189.5010279357393\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2613.039376\n",
            "---------------------------------------\n",
            "Total Timesteps: 456216 Episode Num: 498 Reward: 2418.017869082029\n",
            "Total Timesteps: 457216 Episode Num: 499 Reward: 2407.499626460004\n",
            "Total Timesteps: 458216 Episode Num: 500 Reward: 2457.193640933869\n",
            "Total Timesteps: 459216 Episode Num: 501 Reward: 2425.2985658567663\n",
            "Total Timesteps: 460216 Episode Num: 502 Reward: 2378.5477765733904\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2539.743154\n",
            "---------------------------------------\n",
            "Total Timesteps: 461216 Episode Num: 503 Reward: 2523.6185859591415\n",
            "Total Timesteps: 462216 Episode Num: 504 Reward: 2501.6994950088692\n",
            "Total Timesteps: 463216 Episode Num: 505 Reward: 2491.942815757077\n",
            "Total Timesteps: 464216 Episode Num: 506 Reward: 2515.863103043251\n",
            "Total Timesteps: 465216 Episode Num: 507 Reward: 2430.2513690624464\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2580.950568\n",
            "---------------------------------------\n",
            "Total Timesteps: 466216 Episode Num: 508 Reward: 2530.8756136035963\n",
            "Total Timesteps: 467216 Episode Num: 509 Reward: 2426.078105584565\n",
            "Total Timesteps: 468216 Episode Num: 510 Reward: 2539.6484578006543\n",
            "Total Timesteps: 469216 Episode Num: 511 Reward: 2534.5574570645\n",
            "Total Timesteps: 470216 Episode Num: 512 Reward: 2507.504776285429\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2408.174363\n",
            "---------------------------------------\n",
            "Total Timesteps: 471216 Episode Num: 513 Reward: 2399.315918774488\n",
            "Total Timesteps: 472216 Episode Num: 514 Reward: 2466.8672131914677\n",
            "Total Timesteps: 473216 Episode Num: 515 Reward: 2528.728978483967\n",
            "Total Timesteps: 474216 Episode Num: 516 Reward: 2467.458435633402\n",
            "Total Timesteps: 475216 Episode Num: 517 Reward: 2498.2919751028912\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2514.785171\n",
            "---------------------------------------\n",
            "Total Timesteps: 476216 Episode Num: 518 Reward: 2387.668531126683\n",
            "Total Timesteps: 477216 Episode Num: 519 Reward: 2409.299281478246\n",
            "Total Timesteps: 478216 Episode Num: 520 Reward: 2441.364341392589\n",
            "Total Timesteps: 479216 Episode Num: 521 Reward: 2377.185743706427\n",
            "Total Timesteps: 480216 Episode Num: 522 Reward: 2488.3342796712436\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2427.962110\n",
            "---------------------------------------\n",
            "Total Timesteps: 481216 Episode Num: 523 Reward: 2418.1365152681547\n",
            "Total Timesteps: 482216 Episode Num: 524 Reward: 2548.6017976920416\n",
            "Total Timesteps: 483216 Episode Num: 525 Reward: 2477.059450758192\n",
            "Total Timesteps: 484216 Episode Num: 526 Reward: 2466.1245173199136\n",
            "Total Timesteps: 485216 Episode Num: 527 Reward: 2597.582637453562\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2552.733652\n",
            "---------------------------------------\n",
            "Total Timesteps: 486216 Episode Num: 528 Reward: 2521.6892443799998\n",
            "Total Timesteps: 487216 Episode Num: 529 Reward: 2411.7675364529828\n",
            "Total Timesteps: 488216 Episode Num: 530 Reward: 2397.224774938503\n",
            "Total Timesteps: 489216 Episode Num: 531 Reward: 2435.1009729155703\n",
            "Total Timesteps: 490216 Episode Num: 532 Reward: 2419.0627091241463\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2602.614698\n",
            "---------------------------------------\n",
            "Total Timesteps: 491216 Episode Num: 533 Reward: 2549.0705780289504\n",
            "Total Timesteps: 492216 Episode Num: 534 Reward: 2494.9311321161726\n",
            "Total Timesteps: 493216 Episode Num: 535 Reward: 2501.9894051235333\n",
            "Total Timesteps: 494216 Episode Num: 536 Reward: 2598.986981841969\n",
            "Total Timesteps: 495216 Episode Num: 537 Reward: 2594.1133396319287\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2551.365753\n",
            "---------------------------------------\n",
            "Total Timesteps: 496216 Episode Num: 538 Reward: 2540.8354546226014\n",
            "Total Timesteps: 497216 Episode Num: 539 Reward: 2488.3176361753262\n",
            "Total Timesteps: 498216 Episode Num: 540 Reward: 2524.4337887617226\n",
            "Total Timesteps: 499216 Episode Num: 541 Reward: 2341.002445831747\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2579.716509\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "b0bbcad6-5687-47d6-b98a-311b54edd33d"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2572.869335\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "b29b0116-0e29-4c74-ca48-2a41424faa19"
      },
      "source": [
        "!ls -l exp/brs/monitor"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2444\n",
            "-rw-r--r-- 1 root root    2028 Apr  7 07:28 openaigym.video.0.129.video000000.meta.json\n",
            "-rw-r--r-- 1 root root    5906 Apr  7 07:28 openaigym.video.0.129.video000000.mp4\n",
            "-rw-r--r-- 1 root root    2028 Apr  7 07:28 openaigym.video.0.129.video000001.meta.json\n",
            "-rw-r--r-- 1 root root 1239324 Apr  7 07:28 openaigym.video.0.129.video000001.mp4\n",
            "-rw-r--r-- 1 root root    2028 Apr  7 07:29 openaigym.video.0.129.video000008.meta.json\n",
            "-rw-r--r-- 1 root root 1240389 Apr  7 07:29 openaigym.video.0.129.video000008.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7UreI5OjamR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "229cd853-27e6-4154-f983-6bce127b8d17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFgsw66jnWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"/content/exp/brs/monitor/openaigym.video.0.129.video000000.meta.json\" \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
        "!cp -r \"/content/exp/brs/monitor/openaigym.video.0.129.video000000.mp4\" \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
        "!cp -r \"/content/exp/brs/monitor/openaigym.video.0.129.video000001.meta.json\" \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
        "!cp -r \"/content/exp/brs/monitor/openaigym.video.0.129.video000001.mp4\" \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
        "!cp -r \"/content/exp/brs/monitor/openaigym.video.0.129.video000008.meta.json\" \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
        "!cp -r \"/content/exp/brs/monitor/openaigym.video.0.129.video000008.mp4\" \"/content/gdrive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}