{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2Session9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtQAu81j0cR7dJJRGvTiqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Curiousss/EVA/blob/master/Phase2/Session9/P2Session9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ODn-6LJ57uh"
      },
      "source": [
        "  \n",
        "  INITIALIZATION  We initialize the Experience Replay Memory, with a size of 20000. We will populate it with each new transition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv10juJZ5v54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pybullet envs \n",
        "import gym \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable \n",
        "from collections import deque\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbQKbhvnkECH",
        "colab_type": "text"
      },
      "source": [
        "We initialize the Experience Replay Memory with a size of `max_size` 1e6.\n",
        "Then we populate it with new transitions.\n",
        "Allow the Agent to run randomly and fill up the Replay Buffer `addTransition`. \n",
        "We need the Replay Buffer/Memory ready before training the Critic.\n",
        "We run full episodes with the first 10,000 actions played randomly, and then with actions played by the Actor Model. This is required to fill up the Replay Memory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfHCc-Ezxaf-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The replay buffer, `storage` is an array and the `ptr` is moved from postion 0 to end as and when the trasitions are added, after reaching the end that is `max_size`, the pointer resets to 0 and the new transitions are over-written and this continues.\n",
        "\n",
        "`sample` samples a `batch_size` of data randomly from the `storage` replay buffer. the length of `storage` maybe less than or equal to `max_size` while `storage` replay buffer is still getting updated.\n",
        "`batch_dones` is a `done` switch for each batch to check if the episode is done completely or not.\n",
        "\n",
        "Make any array each for `batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones`from element of the tuple (`state, next_state, action, reward, done`) which the tranistion from each entry in the `storage`. There will no duplicate copies of these entries but multiple references to the memory location is used. Now return these new arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szgjTCTnj886",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  # Init is there for all classes to initialize an object\n",
        "  # Self is pointer to the object of the class which is initialized\n",
        "  def __init_(self, max_size = 1e6):\n",
        "    self.storage =[] \n",
        "    self.max_size = max_size \n",
        "    self.ptr = 0\n",
        "\n",
        "def add(self, transition):\n",
        "  if len(self.storage) == self.max_size:\n",
        "    self.storage[int(self.ptr)] = transition \n",
        "    self.ptr = (self.ptr + 1) % self.max_size\n",
        "  else:\n",
        "    self.storage.append(transition)\n",
        "\n",
        "def sample(self, batch_size):\n",
        "  ind = np.random.randint (e, len(self.storage), batch_size) \n",
        "  batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], [] \n",
        "  \n",
        "  for i in ind:\n",
        "    state, next_state, action, reward, done = self.storage[i] \n",
        "    batch_states.append(np.array(state, copy = False)) \n",
        "    batch_next_states.append(np.array(next_state, copy = False)) \n",
        "    batch_actions.append(np.array(action, copy - False)) \n",
        "    batch_rewards.append(np.array(reward, copy - False)) \n",
        "    batch_dones.append(np.array(done, copy - False) \n",
        "  return np. array (batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), \\\n",
        "          np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To9r3c9i6VuL",
        "colab_type": "text"
      },
      "source": [
        "We build TWO kinds of actor models. One called the Actor Model and another called the Actor Target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrR7vSzausIv",
        "colab_type": "text"
      },
      "source": [
        "Build one DNN is defined to be used for both the Actor model and the Actor Target.\n",
        "The Actor Model is trained using Back Propagation while training on Experience Replay buffer.\n",
        "The Actor Target is trained using the updates from Actor Model through Polyak Averaging, stabilization algorithm. Using Polyak averaging, our targets are not as reactive to changes as models. Targets are delayed in their training as well, which means we allow our models to stabilize first before we update targets. \n",
        "\n",
        "So they get similar to the model only if the model is consistent.   \n",
        "Actor Model and Actor Target models have exactly the same DNN definitions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1QcBXUDzwLE",
        "colab_type": "text"
      },
      "source": [
        "`state_dims` is the state parameters defining the number of variables in a state. These variables define the state in the environment. Eg. Where is the arm of the  robot\n",
        "\n",
        "`action_dim` is the number parameters to define the actions that can be taken. Value of each action (eg. Right Left) is a floating point number representing the quantity of a particular action(Eg. 5 degree to the left).\n",
        "\n",
        "`max_action` is the limit of this quantity of an action (Eg. 90 degrees)\n",
        "\n",
        "The `forward` function outputs `x` the actions, which is scaled to -1 to 1 using tanh activation and multipled by `max_action` to get the exact quantity of each action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWn_h12AxRZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor (nn. Module):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    #Max action is to clip in case we added too much noise\n",
        "    super(Actor, self).__init__# activate the inheritance \n",
        "    self.layer_1 = nn.Linear(state_dims, 400) \n",
        "    self.layer_2 = nn.Linear(400, 300) \n",
        "    self.layer_3 = nn.Linear(300, action_dim) \n",
        "    self.max_action = max_action\n",
        "\n",
        "def forward(self,x):\n",
        "  x = F.relu(self.layer_1(x)) \n",
        "  x = F.relu(self.layer_2(x)) \n",
        "  x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9I4PMY8nKsm",
        "colab_type": "text"
      },
      "source": [
        "STEP 3 We define 1 DNN for Critic 1 and 2. \n",
        "Critic 1 and 2 are just different set of layers in a neural network. `Forward` function returns 2 outputs from 2 different layers from critic 1 and 2. \n",
        "We Build 2 DNNs, for the two Critic models and two for the two Critic Targets\n",
        "\n",
        "We do not have `max_action` in Critic since action is coming from Actor DNN where it is taken care.\n",
        "\n",
        "Critic takes state `x` and action `u` and concatenates it(vertically) before sending it to the DNN layers\n",
        "\n",
        "The Q1 function is a separate forward function that does not back propagate. It is used to train the Actor using the layers of Critic 1. It does not matter whether critic 1 or 2 is used here in the long run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbVanHmayrv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dims, action_dim):\n",
        "    #max_action is to clip in case we need added too much noise\n",
        "    super(Critic, sell).__init__() #activate the inheritence\n",
        "    #First Critic Network\n",
        "    self.layer_1 = nn.Linear(state_dims + action_dim, 400) \n",
        "    self.layer_2 = nn.Linear(400, 300) \n",
        "    self.layer_3 = nn.Linear (300, action_dim) \n",
        "    #Second Critic Network \n",
        "    self.layer_4 = nn.Linear(state_dims + action_dim, 400) \n",
        "    self.layer_5 = nn.Linear(400, 300) \n",
        "    self.layer_6 = nn.Linear(300, action_dim)\n",
        "\n",
        "def forward(self, x, u): \n",
        "  # x is state, u is action \n",
        "  xu = torch.cat([x, u], 1) # 1 for vertical concatenation, 0 for Horizontal\n",
        "  #forward propagation on first Critic\n",
        "  x1 = F.relu(self.layer_1(xu)) \n",
        "  x1 = F.relu(self.layer_2(x1)) \n",
        "  x1 = self.layer_3(x1) \n",
        "  # forward propagation on second Critic \n",
        "  x2 = F.relu(self.layer_4(xu)) \n",
        "  x2 = F.relu(self.layer_5(x2))\n",
        "  x2 = self.layer_6(x2)\n",
        "  return x1, x2\n",
        "\n",
        "def Q1(self, x, u): \n",
        "  #x state, u- action This is used for updating the Q values \n",
        "  xu = torch.cat([x, u], 1) # 1 for vertical concatenation, 0 for horizontal\n",
        "  x1 = F.relu(self.layer_1(xu))\n",
        "  x1 = F.relu(self.layer_2(x1)\n",
        "  x1 = self.layer_3(x1)\n",
        "  return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmqBSVS0m4lG",
        "colab_type": "text"
      },
      "source": [
        "Training process. Create a T3D class, initialize variables and get ready for step 4\n",
        "\n",
        "The `actor` is the Actor Model that is trained using the backpropagation.\n",
        "The `actor` and `actor_target` are initialized using `state_dims, action_dim, max_action`, same as explained earlier.\n",
        "\n",
        "The weights of the `actor_target` is initialized to the weight of the `actor`. This is updated using Polyak Averaging.\n",
        "\n",
        "Similarly...\n",
        "\n",
        "The `critic` is the Critic Model that is trained using the backpropagation.\n",
        "The `critic` and `critic_target` are initialized using `state_dims, action_dim`, same as explained earlier.\n",
        "\n",
        "The weights of the `critic_target` is initialized to the weight of the `critic`. This is updated using Polyak Averaging. \n",
        "\n",
        "`select_action` takes the current state(converted to tensor) calls the Actor Model(forward function) to get the action(converted to numpy).The output action that is sent to the Critic forward function `Q1` explained earlier. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yq9aW9OyO4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is avallable() else cpu)\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "class T3D(object):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    # making sure out T3D can work with any environment\n",
        "    self.actor = Actor(state_dims, action_dim, max_action).to(device) #GD\n",
        "    self.actor_target = Actor(state_dims, action_dim, max_action).to(device) #Polyak Averaging\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "    #Initializing with model weights to keep them same\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor parameters()\n",
        "    \n",
        "    self.critic = Critic(state_dims, action_dim).to(device) #GD \n",
        "    self.critic_target = critic(state_dims, action_dim).to(device) #Polyak Averaging \n",
        "    self.critic_target.load_state_dict(self.critic.state_dict) \n",
        "    #Initializing with model weights to keep the same \n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters()) \n",
        "    self.max_action = max_action\n",
        "\n",
        "def select_action(self, state):\n",
        "  state = torch.Tensor(state.reshape(1, -1)).to(device) \n",
        "  return self.actor(state).cpu().data.numpy().flatten() \n",
        "  # Need to convert to numpy remember clipping!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1PoyxmQnI_h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Sample from a batch of transitions (s, s', a, r) from the memory\n",
        "\n",
        "* `replay_buffer` is replay buffer object defined earlier, \n",
        "* `iterations` is number of iterations of the training, \n",
        "* `batch_size`=100 number of transitions in an input batch, \n",
        "* `discount`=0.99 is the discounting factor in the bellman Equation,\n",
        "* `tau` = 0.005 is a parameter for Polyak Averaging, \n",
        "* `policy_noise` = 0.2 noise added to the actions, \n",
        "* `noise_clip`=0.5 maximum quantity of the action based on the given environment, \n",
        "* `policy_freq`=2 how often the Actor is updated\n",
        "\n",
        "For the number of `iterations` defined repeat:\n",
        "* Get an array of sample of transitions from `replay_buffer` of size `batch_size`\n",
        "* The sample is the set of arrays `batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones` as explained earlier.\n",
        "* Convert each of them to a Tensor as needed by the DNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETjWtrLznINv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, \\\n",
        "          tau = 0.005, policy_noise = 0.2, noise_clip=0.5, policy_freq=2):\n",
        "  for it in range(iterations):\n",
        "    # Step 4 We sample from a batch of transitions (s, s', a, r) from memory \n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "    state = torch. Tensor(batch_states).to(device) \n",
        "    next_state = torch.Tensor(batch_next_states).to(device) \n",
        "    action = torch.Tensor(batch_actions).to(device)\n",
        "    reward = = torch.Tensor(batch_rewards).to(device)\n",
        "    done = torch.Tensor (batch_dones).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVQ5_it41Ccw",
        "colab_type": "text"
      },
      "source": [
        "Step 5: \n",
        "From the next state s\\`, the Actor target plays the next action a\\`. Both are used later by the Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdj7e07SxZgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "self.actor_target.forward(next_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC6nIEpw1KnD",
        "colab_type": "text"
      },
      "source": [
        "We add Gaussian noise to this next action a'. This is the same as exploration!\n",
        "The noise is calculated by applying `normal_` function from `Torch` on the `batch_actions` which is the array of all actions in the given sample. The mean is 0 and standard deviation is `policy_noise` which is a hyper parameter.\n",
        "Then we clamp the noise to a set limit using `noise_clip` which is a hyper parameter.\n",
        "After applying this noise to `next_action` the `next_action` is also clamped in a range of action values supported by the environment using `max_actions`\n",
        "Step 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQaYKDUjxel2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) \n",
        "noise = noise.clamp(-noise_clip, noise_clip) \n",
        "next_action = (next_action + noise).clamp(-self.max_action, self.max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgRy5zeN15Ry",
        "colab_type": "text"
      },
      "source": [
        " STEP 7 \n",
        "The two Critic targets take each the couple (s', a') the `next_state` and `next_action` calculated earlier as input and return two Q values,\n",
        "`target_Q1`(s', a') and `target_Q2`(s', a') as outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgDMTR-emf9p",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOL5BvgXxm0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NaBn-DH2dU3",
        "colab_type": "text"
      },
      "source": [
        " STEP 8 Keep the minimum of these two Q-Values. his is not target_Q, we are just being lazy, and want to use the same variable name later on. \n",
        "\n",
        "It represents the approximated values of the next state.\n",
        "Taking a minimum of the 2 Q-values prevents too optimistic estimates of that value of the state!\n",
        "\n",
        "In classic Actor-Critic Method (with 1 Critic) we had overly optimistic estimates which prevented the training process from being stable, and taking the minimum of 2 Q-Values here adds that stability which was required. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYSMM03jxuNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_Q = torch.min(target_Q1, target_Q2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH-EBAHV25Or",
        "colab_type": "text"
      },
      "source": [
        " STEP 9 \n",
        "We get the final target of the two Critic models, which is:\n",
        "\n",
        "Qt = `reward` + `discount`  * `Qt`\n",
        "\n",
        "`discount` is gamma\n",
        "\n",
        "`Qt = min(Qt1, Qt2)`\n",
        "\n",
        "BUT... We are run this only if the episode is not over that is `Done = 0` which is integrated in the above equation with `(1-done)`.\n",
        "\n",
        "We use Pytorch detach to copy the value or make a branch to avoid target_Q to create it's computation graph hence avoid backprop on itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyoYqPaexzdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_Q = reward + ((1-done) * discount * target_Q).detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zqL6NTp3Sc_",
        "colab_type": "text"
      },
      "source": [
        " STEP 10 \n",
        "Two critic models take (s, a) the current state and current action and return two Q-Values respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYEva44hx4IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_Q1, current_Q2 = self.critic.forward(state, action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yErAgpZM3bom",
        "colab_type": "text"
      },
      "source": [
        " STEP 11 \n",
        " \n",
        " We compute the loss coming from the two Critic models: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY4t6kWax-js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "critic_loss = Mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtZzgcMQ3r98",
        "colab_type": "text"
      },
      "source": [
        " STEP 12 \n",
        "Backpropagate this critic loss and update the parameters of two\n",
        "Critic models with Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoX-P4lkyDvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selt.critic_optimizer.zero_grad() #Initialize the gradients to zero\n",
        "critic_loss.backward() #Computing the gradients\n",
        "self.critic_optimizer.step() #Performing the weight updates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQw5vgx37mV",
        "colab_type": "text"
      },
      "source": [
        " STEP 13 \n",
        "Once every two iterations defined by `policy_freq`, we update our Actor model by performing gradient ASCENT on the output of the first Critic model. \n",
        "* First call Actor Model with Current state to get current action.\n",
        "* Call the Q1 forward funtion of Critic(that does not backprop on Critic) send Current state and current actionusing to get the Q value.\n",
        "* Take a mean of that Q value (of all the Asynchonous Actors of a batch)\n",
        "* Take the negative of the mean to perform gradient ascent and Backprop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDh9paqEyOBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if it % policy_freq == 0:\n",
        "  #This is DPG part \n",
        "  actor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "  self.actor_optimizer.grad_zero() \n",
        "  actor_loss.backward() \n",
        "  self.actor_optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MSx3k124Xnb",
        "colab_type": "text"
      },
      "source": [
        " STEP 14 \n",
        "By now the Critics are updated 4 times.\n",
        "\n",
        "Once in every two iterations, we update our Actor Target by Polyak Averaging.\n",
        "\n",
        "The parameters in Actor Model and Actor Target are defined from same Actor class so each parameter can be paired respectively and iteratied using `zip` and `for` loop.\n",
        "\n",
        "* For each parameter in Actor Model `param` and Actor Target `target_param`, update parameters of Actor Target directly using the respective parameter of the Actor Model using the  Polyak Averaging Equation:\n",
        "\n",
        "* target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data) \n",
        "\n",
        "Using the data part of the parameter Tensors.\n",
        "\n",
        "Where tau \n",
        "\n",
        "Now, why do we have Different Actor Target and Actor Models? \n",
        "Well, they can be the same, and in fact, in many naive RL models, they are the same. \n",
        "But we can improve overall performance by keeping two models and updating them from each other. \n",
        "Once every two iterations, we update the weights of the Actor target by Polyak averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H974X1syVT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param, target_param in zip(self.actor.parameters (), self.actor_target.parameters()):\n",
        "  target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDRM2c6GavQ",
        "colab_type": "text"
      },
      "source": [
        "This way our target comes closer to the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDE1jGJP4nc9",
        "colab_type": "text"
      },
      "source": [
        " STEP 15 \n",
        "In once every two iterations, we update our Critic Target by Polyak Averaging\n",
        "Similar to the way the Actor Target is updated as explained in the previous step.\n",
        "\n",
        "Where is the DELAYED part of the T3D?\n",
        "We update our models at every step, but our target once every two steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1cMtyzyciP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param, target_param in zip(self.critical.parameters(), self.critic_target.parameters()):\n",
        "  target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}