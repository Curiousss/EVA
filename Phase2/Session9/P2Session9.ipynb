{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2Session9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkjCZbCZuoiPkGvWK4yXMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Curiousss/EVA/blob/master/Phase2/Session9/P2Session9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ODn-6LJ57uh"
      },
      "source": [
        "  \n",
        "  INITIALIZATION  We initialize the Experience Replay Memory, with a size of 20000. We will populate it with each new transition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv10juJZ5v54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pybullet envs \n",
        "import gym \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable \n",
        "from collections import deque\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbQKbhvnkECH",
        "colab_type": "text"
      },
      "source": [
        "We initialize the Experience Replay Memory with a size of `max_size` 1e6.\n",
        "Then we populate it with new transitions.\n",
        "Allow the Agent to run randomly and fill up the Replay Buffer `addTransition`. \n",
        "We need the Replay Buffer/Memory ready before training the Critic.\n",
        "We run full episodes with the first 10,000 actions played randomly, and then with actions played by the Actor Model. This is required to fill up the Replay Memory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfHCc-Ezxaf-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The replay buffer, `storage` is an array and the `ptr` is moved from postion 0 to end as and when the trasitions are added, after reaching the end that is `max_size`, the pointer resets to 0 and the new transitions are over-written and this continues.\n",
        "\n",
        "`sample` samples a `batch_size` of data randomly from the `storage` replay buffer. the length of `storage` maybe less than or equal to `max_size` while `storage` replay buffer is still getting updated.\n",
        "`batch_dones` is a `done` switch for each batch to check if the episode is done completely or not.\n",
        "\n",
        "Make any array each for `batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones`from element of the tuple (`state, next_state, action, reward, done`) which the tranistion from each entry in the `storage`. There will no duplicate copies of these entries but multiple references to the memory location is used. Now return these new arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szgjTCTnj886",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  # Init is there for all classes to initialize an object\n",
        "  # Self is pointer to the object of the class which is initialized\n",
        "  def __init_(self, max_size = 1e6):\n",
        "    self.storage =[] \n",
        "    self.max_size = max_size \n",
        "    self.ptr = 0\n",
        "\n",
        "def add(self, transition):\n",
        "  if len(self.storage) == self.max_size:\n",
        "    self.storage[int(self.ptr)] = transition \n",
        "    self.ptr = (self.ptr + 1) % self.max_size\n",
        "  else:\n",
        "    self.storage.append(transition)\n",
        "\n",
        "def sample(self, batch_size):\n",
        "  ind = np.random.randint (e, len(self.storage), batch_size) \n",
        "  batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], [] \n",
        "  \n",
        "  for i in ind:\n",
        "    state, next_state, action, reward, done = self.storage[i] \n",
        "    batch_states.append(np.array(state, copy = False)) \n",
        "    batch_next_states.append(np.array(next_state, copy = False)) \n",
        "    batch_actions.append(np.array(action, copy - False)) \n",
        "    batch_rewards.append(np.array(reward, copy - False)) \n",
        "    batch_dones.append(np.array(done, copy - False) \n",
        "  return np. array (batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), \\\n",
        "          np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To9r3c9i6VuL",
        "colab_type": "text"
      },
      "source": [
        "We build TWO kinds of actor models. One called the Actor Model and another called the Actor Target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrR7vSzausIv",
        "colab_type": "text"
      },
      "source": [
        "Build one DNN is defined to be used for both the Actor model and the Actor Target.\n",
        "The Actor Model is trained using Back Propagation while training on Experience Replay buffer.\n",
        "The Actor Target is trained using the updates from Actor Model through Polyak Averaging, stabilization algorithm. Using Polyak averaging, our targets are not as reactive to changes as models. Targets are delayed in their training as well, which means we allow our models to stabilize first before we update targets. \n",
        "\n",
        "So they get similar to the model only if the model is consistent.   \n",
        "Actor Model and Actor Target models have exactly the same DNN definitions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1QcBXUDzwLE",
        "colab_type": "text"
      },
      "source": [
        "`state_dims` is the state parameters defining the number of variables in a state. These variables define the state in the environment. Eg. Where is the arm of the  robot\n",
        "\n",
        "`action_dim` is the number of actions that can be taken. Value of each action (eg. Right Left) is a floating point number representing the quantity of a particular action(Eg. 5 degree to the left).\n",
        "\n",
        "`max_action` is the limit of this quantity of an action (Eg. 90 degrees)\n",
        "\n",
        "The `forward` function outputs `x` the actions, which is scaled to -1 to 1 using tanh activation and multipled by `max_action` to get the exact quantity of each action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWn_h12AxRZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor (nn. Module):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    #Max action is to clip in case we added too much noise\n",
        "    super(Actor, self).__init__# activate the inheritance \n",
        "    self.layer_1 = nn.Linear(state_dims, 400) \n",
        "    self.layer_2 = nn.Linear(400, 300) \n",
        "    self.layer_3 = nn.Linear(300, action_dim) \n",
        "    self.max_action = max_action\n",
        "\n",
        "def forward(self,x):\n",
        "  x = F.relu(self.layer_1(x)) \n",
        "  x = F.relu(self.layer_2(x)) \n",
        "  x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9I4PMY8nKsm",
        "colab_type": "text"
      },
      "source": [
        "STEP 3 We define 1 DNN for Critic 1 and 2. \n",
        "Critic 1 and 2 are just different set of layers in a neural network. `Forward` function returns 2 outputs from 2 different layers from critic 1 and 2. \n",
        "We Build 2 DNNs, for the two Critic models and two for the two Critic Targets\n",
        "\n",
        "We do not have `max_action` in Critic since action is coming from Actor DNN where it is taken care.\n",
        "\n",
        "Critic takes state `x` and action `u` and concatenates it(vertically) before sending it to the DNN layers\n",
        "\n",
        "The Q1 function is a separate forward function that does not back propagate. It is used to train the Actor using the layers of Critic 1. It does not matter whether critic 1 or 2 is used here in the long run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbVanHmayrv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dims, action_dim):\n",
        "    #max_action is to clip in case we need added too much noise\n",
        "    super(Critic, sell).__init__() #actvate the inheritence\n",
        "    #First Critic Network\n",
        "    self.layer_1 = nn.Linear(state_dims + action_dim, 400) \n",
        "    self.layer_2 = nn.Linear(400, 300) \n",
        "    self.layer_3 = nn.Linear (300, action_dim) \n",
        "    #Second Critic Network \n",
        "    self.layer_4 = nn.Linear(state_dims + action_dim, 400) \n",
        "    self.layer_5 = nn.Linear(400, 300) \n",
        "    self.layer_6 = nn.Linear(300, action_dim)\n",
        "\n",
        "def forward(self, x, u): \n",
        "  # x is state, u is action \n",
        "  xu = torch.cat([x, u], 1) # 1 for vertical concatenation, 0 for Horizontal\n",
        "  #forward propagation on first Critic\n",
        "  x1 = F.relu(self.layer_1(xu)) \n",
        "  x1 = F.relu(self.layer_2(x1)) \n",
        "  x1 = self.layer_3(x1) \n",
        "  # forward propagation on second Critic \n",
        "  x2 = F.relu(self.layer_4(xu)) \n",
        "  x2 = F.relu(self.layer_5(x2))\n",
        "  x2 = self.layer_6(x2)\n",
        "  return x1, x2\n",
        "\n",
        "def Q1(self, x, u): \n",
        "  #x state, u- action This is used for updating the Q values \n",
        "  xu = torch.cat([x, u], 1) # 1 for vertical concatenation, 0 for horizontal\n",
        "  x1 = F.relu(self.layer_1(xu))\n",
        "  x1 = F.relu(self.layer_2(x1)\n",
        "  x1 = self.layer_3(x1)\n",
        "  return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmqBSVS0m4lG",
        "colab_type": "text"
      },
      "source": [
        "Training process. Create a T3D class, initialize variables and get ready for step 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yq9aW9OyO4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is avallable() else cpu)\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "class T3D(object):\n",
        "  def __init__(self, state_dims, action_dim, max_action):\n",
        "    # making sure out T3D can work with any environment\n",
        "    self.actor = Actor(state_dias, action_din, max_action).to(device) #GD\n",
        "    self.actor_target = Actor(state_dims, action_dim, max_action).to(device) #Polyak Averaging\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "    #Initializing with model weights to keep them same\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor parameters()\n",
        "    \n",
        "    self.critic = Critic(state_dims, action_dim).to(device) #GD \n",
        "    self.critic_target = critic(state_dims, action_dim).to(device) #Polyak Averaging \n",
        "    self.critic_target.load_state_dict(self.critic.state_dict) \n",
        "    #Initializing with model weights to keep the same \n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters()) \n",
        "    self.max_action = max_action\n",
        "\n",
        "def select_action(self, state):\n",
        "  state = torch.Tensor(state.reshape(1, -1)).to(device) \n",
        "  return self.actor(state).cpu().data.numpy().flatten() \n",
        "  # Need to convert to numpy remember clipping!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1PoyxmQnI_h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Sample from a batch of transitions (s, s', a, r) from the memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETjWtrLznINv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, \\\n",
        "          tau = 0.005, policy_noise = 0.2, noise_clip=0.5, policy_freq=2):\n",
        "  for it in range(iterations):\n",
        "    # Step 4 We sample from a batch of transitions (s, s', a, r) from memory \n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "    state = torch. Tensor(batch_states).to(device) \n",
        "    next_state = torch.Tensor(batch_next_states).to(device) \n",
        "    action = torch.Tensor(batch_actions).to(device)\n",
        "    reward = = torch.Tensor(batch_rewards).to(device)\n",
        "    done = torch.Tensor (batch_dones).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVQ5_it41Ccw",
        "colab_type": "text"
      },
      "source": [
        "Step 5: \n",
        "Then from each element of the batch, From the next state s\\`, the Actor target plays the next action a\\`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdj7e07SxZgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "self.actor_target.forward(next_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC6nIEpw1KnD",
        "colab_type": "text"
      },
      "source": [
        "We add Gaussian noise to this next action a' and we clamp it in a range\n",
        "of values supported by the environment. This is the same as exploration!\n",
        "Step 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQaYKDUjxel2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) \n",
        "noise = noise.clamp(-noise_clip, noise_clip) \n",
        "next_action = (next_action + noise).clamp(-self.max_action, self.max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgRy5zeN15Ry",
        "colab_type": "text"
      },
      "source": [
        " STEP 7 \n",
        "The two Critic targets take each the couple (s', a') as input and return two Q values,\n",
        "Qt1(s', a') and Qt2(s', a') as outputs\n",
        "The two Critic Targets each take (s`, a`) as input and return two Q-values as output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgDMTR-emf9p",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOCi33r2mlf_",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOL5BvgXxm0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NaBn-DH2dU3",
        "colab_type": "text"
      },
      "source": [
        " STEP 8 Keep the minimum of these two Q-Values. his is not target_Q, we are just being lazy, and want to use the same variable name later on. \n",
        " \n",
        "Step 8: He reep the chimney of these two values \n",
        "this is not target_Q, we are just being lazy, and want to use the same variable name later on. \n",
        "We keep the minimum of these two Q-values\n",
        "It represents the approximated values of the next state.\n",
        "Taking a minimum of the 2 Q-values prevents too optimistic estimates of that value of the state!\n",
        "\n",
        "In classic Actor-Critic Method (with 1 Critic) we had overly optimistic estimates which prevented the training process from being stable,\n",
        "\n",
        "and taking the minimum of 2 Q-Values here adds that stability which was required. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykQ2rpirm5Td",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHUAAAAXCAYAAAA1OADtAAAE30lEQVRoBe3YjZUtQxQF4CMDMiADIvCTADJAAogAESACRIAIEAEiQARkwPre6n2dqanqufNuvxnPumetXt1dXedv73Oq6t6qq1wRuCJwReBSBL6oqpcvNfI/1X+xquDzzOTVZwC+gD++I+LXtsS+q6ofq+qXqvroDp2H/PzSFo/4XL9X1ddVJe4jBD7sHS4I/buq/jzQ8hsbQSuTqdK/qsrciK5G7M9VZc5jyrtV9VtVfVZVyI18WlXihtul8sJWzHwdKsBTgTrlKGFrFSiAEOfqYMX3K1uRAfOxRAcp9DcXAcgP4Ui5VN6qqj8OsnVpLEt9QKjklQDEd+StBOFWjiNAW/lYjb9/RlHp1j3SV7ZX45qK36eSS0E6R//Lbf+ZBZgOAMqeIB5os07e07v0m0JTTEDek5B61GryTVW5TuJgAQDXO1XF4U9bYD9se5NDy/fbEmvD7/uVrol+SDMnY/YOmzmgfz3j8GPeLFm24+sush6LVMDK+4MTuvMH+Zk3y3OusT+q2BVS8H8yO4cdH3IyQxzHKg/ZEXPGYEJiN6oQ6AM4RfDeNrYihT6d2VKSZY2vPYmNW0nuKR3wjd9ziy7kr/bc+4ZjX4Vbx/+JDYOWvi4hpY8hydUlldeNZonpBx4nVTZXyYSQ2ff4GAuqx+FZUfJhZemisMbfvIr5rqW829h7jl/7+Z7IUcGJseNFp5+IxaoxFPHnrTFmtuHF3o1zRsDsgGVsJPC+pHaC4lxlrURw/WdK5mU16EWSb/2eLhjnAagTaEvIttD1n/Y5uY14jfYybyw6hCI7wk6KUO62wpWkWW7hBsxOKgPGxiBnpKYre+Vlqe2kxnkf64GmkGbfESCe2bfYyM+ZG4eG7aMOGpP2PuYXW/e9i2uG12iHP8v0uAWNfySwlSIMbtnGRpvxfQsbRjqpAXhMekYqPfqd1BDdgVw636KMz9meqvN6jCr7q21pSpJiU+0dMMWlyum69/PBkaSKHVn++IjA5du2rOb03sGXh7j8bnVZUYjxkChm8ed9m3K67e6pqYzMZugcUkPguaR2ouMrd/56cWXcXbdJHGnmSdLp3YVghAJjFKCMeZizRyrbDomdpNHu+J7iRprzSYixyuSgNytYdhTEjT2xGRd7yG7Dp0c26Z/wV8kCRyDAVI0xhoxJzJgAV2PmmJt5AmDLGNveXd3PKkjArE64+RuQbR2AlCzzQFxVMl+zQtkjlS9AyaF31gnJxYM46Inx9ap6e8sbdis7sO37aTetYBXFnthubmAmeCc3LEvSZYyjPgYw34z55jljHGauMfZUHTvmme8yFh3vM6ELlD2xFCPJBUSARfhEdBcdPgNUbF2363gWK/sz3XFuf5dn4nPnX1yRcUW0LM/OAfJILsE7NvpdQaxWgD7vUZ8BPZ5eVwFJVjd9uAEAQGR08Z0AqBcTXavMnuiA0d7e/Nk3+VhZbAPuIwF8iI2fEC42W4q7jqc3E/vpqstn8x9tTGfsdVAPzAqBtFw6YxRJfzIcktjPdpAtZ9RTAHctfaPO7D0/s8So6EaxPSDQPQWUpT95zYiTuzzSzaPd/9w7cixL54iultzYAefo7s3RNQF5b95d3xSH36UK5Ah78efvyCOKLvYe5K5y+170IE6fEyd9qX5OQr6GeUXgisC/CPwDf3dTNk3vOxYAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYSMM03jxuNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_Q = torch.min(target_Q1, target_Q2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH-EBAHV25Or",
        "colab_type": "text"
      },
      "source": [
        " STEP 9 \n",
        "We get the final target of the two Critic models, which is:\n",
        "Qt = r + gamma * min(Qt1, Qt2)\n",
        "o\n",
        "We can define target_q or \n",
        "Qt = reward + discount  * Qt\n",
        "\n",
        "but it won't work...\n",
        "\n",
        "First, we are only supposed to run this if the episode is over(=1), which means we need to integrate Done.\n",
        "\n",
        "Second, target_q would create it's BP/computation graph, and without detaching Qt1/Qt2 from their own graph, we are complicating things, i.e. we need to use detach. Let's look below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyoYqPaexzdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_Q = reward + ((1-done) * discount * target_Q).detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zqL6NTp3Sc_",
        "colab_type": "text"
      },
      "source": [
        " STEP 10 \n",
        "Two critic models take (s, a) and return two Q-Vales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUzR5pqowYl",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMpI_oMOo0xE",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYEva44hx4IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_Q1, current_Q2 = self.critic.forward(state, action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yErAgpZM3bom",
        "colab_type": "text"
      },
      "source": [
        " STEP 11 Compute the Critic Loss\n",
        " We compute the loss coming from the two Critic models: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A54n2YZ_o9ko",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY4t6kWax-js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "critic_loss = Mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtZzgcMQ3r98",
        "colab_type": "text"
      },
      "source": [
        " STEP 12 \n",
        "Backpropagate this critic loss and update the parameters of two\n",
        "Critic models with Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoX-P4lkyDvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selt.critic_optimizer.zero_grad() #Initialize the gradients to zero\n",
        "critic_loss.backward() #Computing the gradients\n",
        "self.critic_optimizer.step() #Performing the weight updates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQw5vgx37mV",
        "colab_type": "text"
      },
      "source": [
        " STEP 13 Once every two iterations, we update our Actor model by performing\n",
        "gradient ASCENT on the output of the first Critic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDh9paqEyOBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if it % policy_freq == 0:\n",
        "  #This is DPG part \n",
        "  actor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "  self.actor_optimizer.grad_zero() \n",
        "  actor_loss.backward() \n",
        "  self.actor_optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MSx3k124Xnb",
        "colab_type": "text"
      },
      "source": [
        " STEP 14 Still, in once every two iterations, we update our Actor Target\n",
        "by Polyak Averaging\n",
        "Now, why do we have Different Actor Target and Actor Models? \n",
        "Well, they can be the same, and in fact, in many naive RL models, they are the same. \n",
        "But we can improve overall performance by keeping two models and updating them from each other. \n",
        "Once every two iterations, we update the weights of the Actor target by Polyak averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrtFAjONGF25",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKkAAAAXCAYAAABwFW8qAAAE5ElEQVRoBe2agZUMQRCGSwaIgAwQASIgAkSADIgAGRABIjgiQASIgAx4H/3vq+2rrunpnVu7a+q9eT3TXdVT9fffVb1zZ7bKisCKwIrAisD+ELhnZl/N7PL+XvlfvumGmT08ssjhxpI+vzSzTxkG983snZl9NrNHTvG1mf10z6d6e9PM3pjZhyr+fcQLQXlvS0gQh5gkLpnZRzO703K80Q+/iPeLmYG75FvhoJ63WoxQAIirZvbLzK4XDfpfbWmf3gMgszmJ/0oV/z6iBePbwYuulUxFJXsejB9CF3zBf3ztERIByRAhMWIL2eEbvPMJsqj9HfxRgYTy0/JiT9iN0QndAA7x+7IFYeeQggWi9I0Ii6JF8/aUPrINlYw1mOOPn2cf9xCPa0ogJVVZVUHEJJuCf7NiAwSL4kWgKC37sd57StgxCARhN3vheU71IBOPkoh3ZQRnbq2H9/GQ7iGZJ1/kG8QkGbD5JIqNFpKHmEuJrClRuQN0dnGYfqWctNkZKzG7kCHKCaK2PG5KjI8fHUgRAibDqgXHEZKyuLwLzFuiNRqZvzXn3H7hRqt7Pwd9kJRM2RIwJlafvNCnjxjZrDpibs2hUuIHZSiSbhl0PnC+6kn/ndMNqRHHWXIxKbsakPzhfYQUxDtCIhbue2PhFfSIP7LdpYVMbxP8fPaHpPyAyjY2JOTyAmYiKVw8J0yMEUpkPS0oh3QZnjPq6CCtM5/OHR0mi6uQ/YkN0ACCXU6r60khBr8uiVWx0/IJhL4sK9QOQyTmnivYTFUckfTZ3Ml30OeMDX7vzYz34iMkEn701eubxaKzJ+XeY80zWDcrya2iwJkMIHThHIsapfWpuAmOgMhM2F/UlflRbxIAqs/c2KMHQPir2GnRTYEL4sKORYrizXwl+/SSlPl7JPIh6svmosL46gIm/jmyzUjKjyIwRUdYq2Kn8d8thv7MKcaPlGpKHo6Qif1uuYj7OVmFkhrFA1j468+jOo9Hv7a1MIzVMZF9W3Fnn2ZYoHSRyqJqgeVDq1WSqP2LnrMY/fziBETPBAKSGSNhjBg8FiKp5985W5GUxZLocDu1a6Rft9gBOm20e5fqq9+bPeOPJ6J0RVIfP4ABpu+Tvm/rONBnIep+njOhnC5JUt4V+RD1ZX75MTCZ8hF94o8qlsbA1eNB4qBiN0s9hlokr0Sp791hPhB/z3w4W59ZvM6+7pUZI9Ipfj/WU34j35mDRZor2EwRQH6OzD/Xn0ifjZT9IJJNFgtjPsuyLjxHyUPz/WnrBWTHQFL6dxVKf09gu75nyl6Z0e9gb8NOFkkpP5M72xu7+1GSCvOWf7ziX5JUHMHPTPCfDR4dq7BTaVeccKM7GXJA5jPD45L9/Jkhc6pnrJX6e2yX0gGIzA92OOfJByWj+W94c3wYJanOe1FigBicJfW1RWde/zF8jo8jutogU7hAPjZ4RmbWAd9flC8FsyotjnAtLVOBLf2+aD58mPKD8/Ou8WM/Wo6pXv6bo+IgYXBBAF08T8Uj+yVaiBT5Vs+tzUabCTiN/t7J5l3HOhCAPP5v/x0mGxWyT6tMbpQO/IbyHX6MP3C/V/dmIEA2hejHKHP/C+oYY1x9LiVw6lf+IQLFMYRf/6NV5BBjWn1KEOCseWyLzVecY/M5WYJ1aEVgRWBF4NQQ+A33UTVJwnYokAAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H974X1syVT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param, target_param in zip(self.actor.parameters (), self.actor_target.parameters()):\n",
        "  target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDRM2c6GavQ",
        "colab_type": "text"
      },
      "source": [
        "This way our target comes closer to the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDE1jGJP4nc9",
        "colab_type": "text"
      },
      "source": [
        " STEP 15 Still, in once every two iterations, we update our Critic Target\n",
        "by Polyak Averaging\n",
        "Same steps for the Critics\n",
        "Once every two iterations, we update the weights of the Critic target by Polyak averaging\n",
        "Where is the DELAYED part of the T3D?\n",
        "We update our models at every step, but our target once every two steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1cMtyzyciP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param, target_param in zip(self.critical.parameters(), self.critic_target.parameters()):\n",
        "  target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}