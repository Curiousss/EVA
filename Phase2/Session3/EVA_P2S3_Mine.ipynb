{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA P2S3_Mine.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Curiousss/EVA/blob/master/Phase2/Session3/EVA_P2S3_Mine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA",
        "colab_type": "text"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('text.txt', 'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb",
        "colab_type": "text"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab_type": "code",
        "outputId": "6adbd511-e8b7-41d2-dcef-0a75b0d12807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY",
        "colab_type": "text"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x): # sigmoid function\n",
        "  return 1 /(1+np.exp(-x)) #REMOVE\n",
        "\n",
        "def dsigmoid(y): # derivative of sigmoid function\n",
        "  return y * (1 - y)\n",
        "\n",
        "def tanh(x): # tanh function\n",
        "  return np.tanh(x)\n",
        "\n",
        "def dtanh(y): # derivative of tanh\n",
        "  return 1 - y * y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGRxcbbJCiVJ",
        "colab_type": "code",
        "outputId": "95ac9845-c66e-497d-e7a2-18bddf025191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(sigmoid(0))\n",
        "print(dsigmoid(sigmoid(0)))\n",
        "print(dtanh(tanh(dsigmoid(sigmoid(0)))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "0.25\n",
            "0.940014848806378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). \n",
        "\n",
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212",
        "colab_type": "text"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V",
        "colab_type": "text"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 4\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_a = Hidden_Layer_size # DELETE\n",
        "size_b = z_size\n",
        "size_c = X_size\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs",
        "colab_type": "text"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI",
        "colab_type": "text"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 5\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 6. \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckDHS23SD5wB",
        "colab_type": "code",
        "outputId": "21655bff-aec2-432f-b8f6-5a8f8b52a66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175, 1)\n",
            "0.0\n",
            "50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S",
        "colab_type": "text"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA",
        "colab_type": "text"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y",
        "colab_type": "text"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV",
        "colab_type": "text"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L",
        "colab_type": "text"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a",
        "colab_type": "text"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK",
        "colab_type": "text"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab_type": "code",
        "outputId": "dec1fe24-5f6a-4b17-d11d-2fe3aec4bf63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "iter = 50001\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD1CAYAAACm0cXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsf8HBxsnV/jOApgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " easi. Brot add hos it China. Ano’lls with that hen the officials did not provide additifes.\n",
            "\n",
            "Evuicher a Doceachen st permonc infection the same way in lible sore throat and test Kost and rllotiths ane \n",
            "----\n",
            "iter 50000, loss 5.002600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 7. \n",
        "\n",
        "Run the above code for at least 50000 iterations **twice** (after you ran once, press Run All from \"Runtime\") making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJkBg6MqTJPE",
        "colab_type": "text"
      },
      "source": [
        "[.](https://blog.varunajayasiri.com/numpy_lstm.html)"
      ]
    }
  ]
}