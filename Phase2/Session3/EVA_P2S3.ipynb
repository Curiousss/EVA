{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA P2S3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Curiousss/EVA/blob/master/Phase2/Session3/EVA_P2S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HAv0DVGlBPW",
        "colab_type": "code",
        "outputId": "16bfc45a-7ac8-417d-87da-50c2a0bf225f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab  import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF5Bq7Czlxr0",
        "colab_type": "code",
        "outputId": "c896ca1b-630b-45a9-cdeb-f933d7b480ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA",
        "colab_type": "text"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('/content/gdrive/My Drive/text.txt', 'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb",
        "colab_type": "text"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab_type": "code",
        "outputId": "9b5e07e1-2e00-412a-be90-c451c33d4112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY",
        "colab_type": "text"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x): # sigmoid function\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "  # write your code here\n",
        "\n",
        "def dsigmoid(y): # derivative of sigmoid function\n",
        "  sig = sigmoid(y)\n",
        "  return sig*(1-sig)# write your code here\n",
        "\n",
        "def tanh(x): # tanh function\n",
        "  return np.tanh(x)# write your code here\n",
        "\n",
        "def dtanh(y): # derivative of tanh\n",
        "  return 1.0 - np.tanh(y)**2# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqDkTtKHyhIe",
        "colab_type": "code",
        "outputId": "c52eb04b-9ea5-46f1-f31e-3094f61eaed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"sigmoid(0)\", sigmoid(0))\n",
        "print(\"dsigmoid(sigmoid(0))\", dsigmoid(sigmoid(0)))\n",
        "print(\"tanh(dsigmoid(sigmoid(0)))\", tanh(dsigmoid(sigmoid(0))))\n",
        "print(\"dtanh(tanh(dsigmoid(sigmoid(0))))\", dtanh(tanh(dsigmoid(sigmoid(0)))))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid(0) 0.5\n",
            "dsigmoid(sigmoid(0)) 0.2350037122015945\n",
            "tanh(dsigmoid(sigmoid(0))) 0.2307710272926824\n",
            "dtanh(tanh(dsigmoid(sigmoid(0)))) 0.9485799654066528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). \n",
        "\n",
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212",
        "colab_type": "text"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V",
        "colab_type": "text"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 4\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_a = Hidden_Layer_size# write your code here\n",
        "size_b = z_size# write your code here\n",
        "size_c = X_size# write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs",
        "colab_type": "text"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f =  sigmoid( np.dot(p.W_f.v, z) + p.b_f.v) # write your code here\n",
        "    i = sigmoid( np.dot(p.W_i.v, z) + p.b_i.v) # write your code here\n",
        "    C_bar = tanh( np.dot(p.W_C.v,z) + p.b_C.v) # write your code here\n",
        "\n",
        "    C = f * C_prev + i*C_bar # write your code here\n",
        "    o = sigmoid( np.dot(p.W_o.v, z) + p.b_o.v)# write your code here\n",
        "    h = o*tanh(C)# write your code here\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v # write your code here\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI",
        "colab_type": "text"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 5\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCwUyQKL0pF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d29b9c09-f418-4f35-c20c-a3eedc57e79f"
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 6. \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5iRxtox1DOz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "daa8280d-3bad-43a4-96f3-64ef95adce1f"
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175, 1)\n",
            "0.0\n",
            "50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * dtanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S",
        "colab_type": "text"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA",
        "colab_type": "text"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y",
        "colab_type": "text"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV",
        "colab_type": "text"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L",
        "colab_type": "text"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a",
        "colab_type": "text"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK",
        "colab_type": "text"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "82970411-53b0-4e0e-b72e-92b77f89cf42"
      },
      "source": [
        "iter = 50000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD1CAYAAACm0cXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " nnit 6helnvrysash   cwaIh D'assUrsa henash,caalniea \n",
            " haftrmh dr2a'vlsutlre0ha t8 ntma\n",
            " aldanfdeahea ofzrbvr nlk ou’stunnneceei7cehaa  t tigis  rtesrouau tsTied  y7cs\n",
            "a  irr idiioo -tfmrm,,y aa nAaekk \n",
            "----\n",
            "iter 49900, loss 123.089390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 7. \n",
        "\n",
        "Run the above code for 50000 iterations making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    }
  ]
}